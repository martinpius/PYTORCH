{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MTL with Attention in Pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYzuZAbU23Ox3YhrxYYbFJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/PYTORCH/blob/main/MTL_with_Attention_in_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE5v5KXET0eT",
        "outputId": "01badb5b-12a6-4892-d4c0-25aa3e46b571"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import torch\n",
        "  print(f\"You are on CoLaB with Pytorch Version: {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\"{type(e)}: {e}\\n>>>>please correct {type(e)} and reload your device\")\n",
        "  COLAB = False\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "def time_fmt(t: float = 123.891)->float:\n",
        "  h = int(t / (60 *60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"{h}: {m:>02}: {s:>05.2f}\"\n",
        "print(f\">>>>>time formating:\\tplease wait....\\n>>>>>time elapsed:\\t{time_fmt()}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "You are on CoLaB with Pytorch Version: 1.8.1+cu101\n",
            ">>>>>time formating:\tplease wait....\n",
            ">>>>>time elapsed:\t0: 02: 03.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwjHWXtrWilU"
      },
      "source": [
        "#In this notbook we are going to train a Machine translation we are going to train a machine translation Model\n",
        "#with an attention mechanism in Pytorch. For demonstration we will apply data from torchtext (Multi30k)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBiiuhMNeP8k"
      },
      "source": [
        "#Importing the required modules:\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "import math, sys, time, random, spacy\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iINtJhAe8bs"
      },
      "source": [
        "#Set the random seed for reproducability and device (gpu to deterministic to avoid errors)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyiSNF-KfJ-y"
      },
      "source": [
        "seed = 2134\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C0AABQIf4g3",
        "outputId": "aabb8c85-db9c-495c-ca43-3b62d0c2c149"
      },
      "source": [
        "#We start by installing and loading the tokenizers for data preprocessing:\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "spacy_de = spacy.load('de_core_news_sm')#restart for effective loading\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (56.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (56.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKpoAIkTgl9s"
      },
      "source": [
        "#define the tokenizer functions to be used in the Field module:\n",
        "def en_tokenizer(text):\n",
        "  return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "def de_tokenizer(text):\n",
        "  return [tok.text for tok in spacy_de.tokenizer(text)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1McG9BlxhXri"
      },
      "source": [
        "#Define the field objects for both languages to be used in Multi30k module:\n",
        "germany = Field(tokenize = de_tokenizer, init_token = '<sos>', eos_token = '<eos>', lower = True)\n",
        "english = Field(tokenize = en_tokenizer, init_token = '<sos>', eos_token = '<eos>', lower = True)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1576sg6DiK4g",
        "outputId": "0a62bbea-a9f4-46e1-fb87-8e3bd9bcc9bb"
      },
      "source": [
        "#Load and preprocess the data using the above fields (we load the data with the help of Multi30k.splits module)\n",
        "train_data, validation_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (germany, english))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.21M/1.21M [00:01<00:00, 961kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46.3k/46.3k [00:00<00:00, 276kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66.2k/66.2k [00:00<00:00, 263kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVpR9-Mqip0A",
        "outputId": "20771845-3091-4e9b-9392-a497b8faeb70"
      },
      "source": [
        "#print out number of examples in each dataset\n",
        "print(f\"Total samples in train dataset: {len(train_data.examples)}\\nTotal samples in validation dataset: {len(validation_data.examples)}\\nTotal samples in test dataset: {len(test_data.examples)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total samples in train dataset: 29000\n",
            "Total samples in validation dataset: 1014\n",
            "Total samples in test dataset: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeecH3bgjYXZ",
        "outputId": "4a622bfe-bbb6-4a06-ef19-1fc178868e21"
      },
      "source": [
        "#Print and examine the first example (paired data) for the training set:\n",
        "print(f\">>>>>The first sample in a train data is a paired sentences:\\n{vars(train_data.examples[0])}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>>>The first sample in a train data is a paired sentences:\n",
            "{'src': ['zwei', 'junge', 'weiÃŸe', 'mÃ¤nner', 'sind', 'im', 'freien', 'in', 'der', 'nÃ¤he', 'vieler', 'bÃ¼sche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLE7ViOxj3cd"
      },
      "source": [
        "#We then build the vocabulary for the train dataset only to allow generalization \n",
        "#We only pick the tokens which appeared at least twice in a tokenized list:\n",
        "germany.build_vocab(train_data, min_freq = 2)\n",
        "english.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh0zb53tkYDC"
      },
      "source": [
        "#We finaly build our iterator object ready to stream-in data during training and validation later after building the model\n",
        "#We use the BucketIterator.splits method to do the proper splitting for us. we consider batch_size of 128\n",
        "batch_size = 128\n",
        "train_iter, validation_iter, test_iter = BucketIterator.splits(\n",
        "    (train_data, validation_data, test_data),\n",
        "    batch_size = batch_size,\n",
        "    device = device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIto1HsclMsO"
      },
      "source": [
        "#We start building our Model step by step by firstly defining the decoder class, Attention class, encoder class and finaly\n",
        "#combining them to get the model class"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_hmM7qklc7_"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, enc_input, enc_embd, enc_hidden, dec_hidden, dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.embedding = nn.Embedding(enc_input, enc_embd)\n",
        "    self.rnn = nn.GRU(enc_embd, enc_hidden, bidirectional = True)\n",
        "    self.fc = nn.Linear(enc_hidden * 2, dec_hidden)\n",
        "  \n",
        "  def forward(self, enc_input):\n",
        "    '''\n",
        "    enc_input shape = [enc_input_len = seq_len, batch_size]\n",
        "    embeded shape = [enc_input_len = seq_len, batch_size, embd_dim]\n",
        "    enc_output shape = [enc_input_len, batch_size, enc_hidden*2]\n",
        "    enc_hidden shape = [num_layers*2, batch_size, enc_hidden_dim]\n",
        "    enc_hidden[-1,:,:] = is the last of the backward rnn\n",
        "    enc_hidden[-2,:,:] = is the last of the forward rnn\n",
        "    #We need to stack backward and forward rnn hidden results for future use in the decoder\n",
        "    '''\n",
        "    embeded = self.dropout(self.embedding(enc_input))\n",
        "    enc_output, enc_hidden = self.rnn(embeded)\n",
        "    hidden = torch.tanh(self.fc(torch.cat((enc_hidden[-2,:,:], enc_hidden[-1,:,:]), dim = 1)))\n",
        "    return enc_output, hidden\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7jbLgT8pa9t"
      },
      "source": [
        "#The attention class: To compute information about the decoder's hidden layer with respect\n",
        "#to the input tokens we define the following class:\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hidden, dec_hidden):\n",
        "    super().__init__()\n",
        "    self.attn = nn.Linear((2*enc_hidden) + dec_hidden, dec_hidden)\n",
        "    self.v = nn.Linear(dec_hidden, 1, bias = False)\n",
        "  \n",
        "  def forward(self, hidden, enc_outputs):\n",
        "    '''\n",
        "    hidden shape = [batch_size, dec_hid_dim]\n",
        "    enc_output shape = [input_len, batch_size, enc_hidden*2]\n",
        "    we repeat the decoder hidden times the length of encoder's input\n",
        "    before concatenating with encoder outputs we need to reshape the encoder's output\n",
        "    '''\n",
        "    batch_size = enc_outputs.shape[1]\n",
        "    enc_len = enc_outputs.shape[0]\n",
        "    hidden = hidden.unsqueeze(1).repeat(1,enc_len,1) # new shape [batch_size, enc_len, dec_hidden_dim]\n",
        "    enc_outputs = enc_outputs.permute(1,0,2) #new shape [batch_size, enc_len, enc_hiden*2]\n",
        "    energy = torch.tanh(self.attn(torch.cat((hidden, enc_outputs), dim = 2))) #shape = [batch_size, enc_len, dec_hidden]\n",
        "    attention = self.v(energy).squeeze(2) #shape = [batch_size, enc_input_len]\n",
        "    return nn.functional.softmax(attention, dim = 1)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gzFEHiwuNnC"
      },
      "source": [
        "#We then defines our decoder class that takes as the input the outputs of the above classes (encoder and attention)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc2L_ZhRudbj"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,output_dim,dec_embd, dec_hidden, enc_hidden, attention, dropout):\n",
        "    super().__init__()\n",
        "    self.attention = attention\n",
        "    self.output_dim = output_dim\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.embedding = nn.Embedding(output_dim,dec_embd)\n",
        "    self.rnn = nn.GRU((enc_hidden * 2) + dec_embd, dec_hidden)\n",
        "    self.fc = nn.Linear((enc_hidden * 2) + dec_embd + dec_hidden, output_dim)\n",
        "  \n",
        "  def forward(self, input, hidden, enc_output):\n",
        "    '''\n",
        "    this method will compute the predictions using as inputs attention, dec_hidden and enc outputs\n",
        "    input shape: [batch_size]\n",
        "    hidden shape : [batch_size, dec_hidden_dim]\n",
        "    enc_output shape : [enc_input_len, batch_size, enc_hidden*2]\n",
        "    '''\n",
        "    input = input.unsqueeze(0) # new shape: [batch_size, 1]\n",
        "    embeded = self.dropout(self.embedding(input)) # shape [batch_size, 1, embeded_dim]\n",
        "    a = self.attention(hidden, enc_output) #shape: [batch_size, enc_input_len]\n",
        "    a = a.unsqueeze(1) # new shape is [batch_size, 1, enc_input_len]\n",
        "    enc_output = enc_output.permute(1,0,2) #new shape [batch_size, enc_input_len, enc_hidden*2]\n",
        "    weighted = torch.bmm(a, enc_output) #shape [1, batch_size, enc_hidden *2]\n",
        "    weighted = weighted.permute(1,0,2) #new shape = [batch_size, 1, enc_hidden*2]\n",
        "    rnn_input = torch.cat((embeded, weighted), dim = 2) #shape = [batch, 1, enc_hidden*2 + embeded_dim]\n",
        "    output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "    #output shape: [dec_out_len = 1, batch_size, dec_hidden*2]\n",
        "    #hidden = [1, batch_size, dec_hidden_dim]\n",
        "    output = output.squeeze(0)\n",
        "    embeded = embeded.squeeze(0)\n",
        "    weighted = weighted.squeeze(0)\n",
        "    #compute the predictions\n",
        "    preds = self.fc(torch.cat((output, weighted, embeded), dim = 1)) #shape = [batch_size, output_dim]\n",
        "    return preds, hidden.squeeze(0)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k4obs1F5hsX"
      },
      "source": [
        "#The model Class: We finally build the model by combining the above classes:"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcpYdx-C6LHE"
      },
      "source": [
        "class MTL(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "  def forward(self, input, target, teacher_force_ratio = 0.5):\n",
        "    '''\n",
        "    input shape: [input_len, batch_size]\n",
        "    target shape: [target_len, batch_size]\n",
        "    We will apply techer force learning technique 50% of times == ground truth else best guess\n",
        "    '''\n",
        "    batch_size = input.shape[1]\n",
        "    target_len = target.shape[0]\n",
        "    target_voc_size = self.decoder.output_dim\n",
        "    #define a storage container to keep the predictions\n",
        "    outputs = torch.zeros(target_len, batch_size, target_voc_size).to(self.device)\n",
        "    enc_outputs, hidden = self.encoder(input)\n",
        "    #Get the first decoder's input = 'sos' token\n",
        "    inp_first = target[0,:]\n",
        "    #We the iterate over the decoder's input length (english list of tokens)\n",
        "    for t in range(1, target_len):\n",
        "      output, hidden = self.decoder(inp_first, hidden, enc_outputs)\n",
        "      outputs[t] = output # storing the prediction at every time step\n",
        "      teacher_force = random.random() < teacher_force_ratio\n",
        "      best_guess = output.argmax(1) #grab the best prediction (with max proba)\n",
        "      input = target[t] if teacher_force else best_guess\n",
        "    return outputs\n",
        "    "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRXOQbas-xby"
      },
      "source": [
        "#Writing the training and validation loops for the model:\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb6qpggW-992"
      },
      "source": [
        "#Hyperparameters to be used in this model\n",
        "input_dim = len(germany.vocab)\n",
        "output_dim = len(english.vocab)\n",
        "enc_embd = 300\n",
        "dec_embd = 300\n",
        "enc_hidden = 512\n",
        "dec_hidden = 512\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5\n",
        "\n",
        "attn = Attention(enc_hidden, dec_hidden)\n",
        "enc = Encoder(input_dim, enc_embd, enc_hidden, dec_hidden, enc_dropout)\n",
        "dec = Decoder(output_dim, dec_embd,dec_hidden, enc_hidden, attn, dec_dropout)\n",
        "model = MTL(enc, dec, device).to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRnKKyIJAtc3",
        "outputId": "d4d11eca-13aa-4e66-ab13-378200f732fe"
      },
      "source": [
        "#Initialize the parameters to random-normal\n",
        "def wt_initializer(model):\n",
        "  for name, par in model.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      nn.init.normal_(par.data, mean = 0, std = 0.01)\n",
        "    else:\n",
        "      nn.init.constant_(par.data, 0)\n",
        "model.apply(wt_initializer)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MTL(\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(7855, 300)\n",
              "    (rnn): GRU(300, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(5893, 300)\n",
              "    (rnn): GRU(1324, 512)\n",
              "    (fc): Linear(in_features=1836, out_features=5893, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCJPhfDzB6KN",
        "outputId": "0b83d4fe-05da-4b30-ec88-349b6588d25f"
      },
      "source": [
        "#Count the number of trainable parameters in the model:\n",
        "def par_counts(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"TOTAL trainable parameters in this model are: {par_counts(model):,} parameters\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOTAL trainable parameters in this model are: 21,585,873 parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sghNvBTtCvV-"
      },
      "source": [
        "#get the loss and optimizers objects\n",
        "lr = 1e-3\n",
        "padded_idx = english.vocab.stoi[english.pad_token]\n",
        "optimizer = optim.Adam(params = model.parameters(), lr = lr)\n",
        "loss_obj = nn.CrossEntropyLoss(ignore_index = padded_idx)\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyFPtqNKD-4X"
      },
      "source": [
        "#We now define our training loop using the following function:\n",
        "def training_loop(model, iterator, optimizer, loss_obj, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for i, batch in enumerate(iterator):\n",
        "    input = batch.src\n",
        "    target = batch.trg\n",
        "    optimizer.zero_grad()\n",
        "    preds = model(input, target)\n",
        "    #target shape: [target_len, batch_size], preds shape: [target_len, batch_size, output_dim]\n",
        "    output_dim = preds.shape[-1]\n",
        "    #reshaping the predictions and select from the 2nd token (ignore 'sos' for loss computation)\n",
        "    preds = preds[1:].view(-1, output_dim) # new shape = [(target_len-1) * batch_size, output_dim]\n",
        "    target = target[1:].view(-1) # new shape = [(target_len - 1 * batch_size)]\n",
        "    loss = loss_obj(preds, target)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return (epoch_loss/len(iterator))\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-wk7lgQGp64"
      },
      "source": [
        "#The validation loop is given by the following function:\n",
        "def validation_loop(model, iterator, loss_obj):\n",
        "  model.eval() #turn off regularizers\n",
        "  epoch_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for i, batch in enumerate(iterator):\n",
        "      input = batch.src\n",
        "      target = batch.trg\n",
        "      preds = model(input, target,0) #no teacher forcing\n",
        "      output_dim = preds.shape[-1]\n",
        "      preds = preds[1:].view(-1, output_dim)\n",
        "      target = target[1:].view(-1)\n",
        "      loss = loss_obj(preds, target)\n",
        "      epoch_loss+=loss.item()\n",
        "  return (epoch_loss / (len(iterator)))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dQrTxbsI9FH",
        "outputId": "5dc5dba6-1995-4c77-d22c-6630d4fb3c28"
      },
      "source": [
        "#We now train the model for 100 epochs: We record both loss and exp(loss) = PPl\n",
        "clip = 1\n",
        "best_val_loss = float('inf') # to save best weights\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  tic = time.time()\n",
        "  print(f\"\\n>>>>Training starts for epoch: {epoch + 1}\\n>>>>Please wait and keep your screen active all the time.....\")\n",
        "  train_loss = training_loop(model, train_iter, optimizer, loss_obj, clip)\n",
        "  valid_loss = validation_loop(model, validation_iter, loss_obj)\n",
        "  if valid_loss < best_val_loss:\n",
        "    best_val_loss = valid_loss\n",
        "    torch.save(model.state_dict(), 'mtl_prack.pt')\n",
        "  toc = time.time()\n",
        "  print(f\"\\n>>>>time elapsed for this epoch is: {time_fmt(toc - tic)}\")\n",
        "  print(f\">>>>train loss: {float(train_loss):.4f}::PPL {math.exp(train_loss):7.4f}\")\n",
        "  print(f\"\\n>>>>validation loss: {float(valid_loss):.4f}::PPL {math.exp(valid_loss):7.4f}\")\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>>Training starts for epoch: 1\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 20.00\n",
            ">>>>train loss: 5.2150::PPL 184.0026\n",
            "\n",
            ">>>>validation loss: 5.0365::PPL 153.9240\n",
            "\n",
            ">>>>Training starts for epoch: 2\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 24.00\n",
            ">>>>train loss: 4.6178::PPL 101.2725\n",
            "\n",
            ">>>>validation loss: 4.4201::PPL 83.1042\n",
            "\n",
            ">>>>Training starts for epoch: 3\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 25.00\n",
            ">>>>train loss: 4.0837::PPL 59.3656\n",
            "\n",
            ">>>>validation loss: 3.6224::PPL 37.4286\n",
            "\n",
            ">>>>Training starts for epoch: 4\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 3.5194::PPL 33.7641\n",
            "\n",
            ">>>>validation loss: 3.2377::PPL 25.4750\n",
            "\n",
            ">>>>Training starts for epoch: 5\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 3.1192::PPL 22.6285\n",
            "\n",
            ">>>>validation loss: 3.0417::PPL 20.9414\n",
            "\n",
            ">>>>Training starts for epoch: 6\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 2.8180::PPL 16.7428\n",
            "\n",
            ">>>>validation loss: 2.9255::PPL 18.6440\n",
            "\n",
            ">>>>Training starts for epoch: 7\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 2.5663::PPL 13.0175\n",
            "\n",
            ">>>>validation loss: 2.9107::PPL 18.3692\n",
            "\n",
            ">>>>Training starts for epoch: 8\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 2.3617::PPL 10.6094\n",
            "\n",
            ">>>>validation loss: 2.8731::PPL 17.6925\n",
            "\n",
            ">>>>Training starts for epoch: 9\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 26.00\n",
            ">>>>train loss: 2.1962::PPL  8.9906\n",
            "\n",
            ">>>>validation loss: 2.9170::PPL 18.4850\n",
            "\n",
            ">>>>Training starts for epoch: 10\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 2.0711::PPL  7.9337\n",
            "\n",
            ">>>>validation loss: 2.9191::PPL 18.5243\n",
            "\n",
            ">>>>Training starts for epoch: 11\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 1.9649::PPL  7.1340\n",
            "\n",
            ">>>>validation loss: 2.9374::PPL 18.8669\n",
            "\n",
            ">>>>Training starts for epoch: 12\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 1.8721::PPL  6.5016\n",
            "\n",
            ">>>>validation loss: 2.9899::PPL 19.8829\n",
            "\n",
            ">>>>Training starts for epoch: 13\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 26.00\n",
            ">>>>train loss: 1.7883::PPL  5.9792\n",
            "\n",
            ">>>>validation loss: 3.0224::PPL 20.5399\n",
            "\n",
            ">>>>Training starts for epoch: 14\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 1.7145::PPL  5.5539\n",
            "\n",
            ">>>>validation loss: 3.0697::PPL 21.5355\n",
            "\n",
            ">>>>Training starts for epoch: 15\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 26.00\n",
            ">>>>train loss: 1.6430::PPL  5.1704\n",
            "\n",
            ">>>>validation loss: 3.1043::PPL 22.2932\n",
            "\n",
            ">>>>Training starts for epoch: 16\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 28.00\n",
            ">>>>train loss: 1.5877::PPL  4.8924\n",
            "\n",
            ">>>>validation loss: 3.1699::PPL 23.8045\n",
            "\n",
            ">>>>Training starts for epoch: 17\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 1.5241::PPL  4.5908\n",
            "\n",
            ">>>>validation loss: 3.2182::PPL 24.9833\n",
            "\n",
            ">>>>Training starts for epoch: 18\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 1.4735::PPL  4.3643\n",
            "\n",
            ">>>>validation loss: 3.2491::PPL 25.7672\n",
            "\n",
            ">>>>Training starts for epoch: 19\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 28.00\n",
            ">>>>train loss: 1.4289::PPL  4.1740\n",
            "\n",
            ">>>>validation loss: 3.2792::PPL 26.5552\n",
            "\n",
            ">>>>Training starts for epoch: 20\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 26.00\n",
            ">>>>train loss: 1.3840::PPL  3.9909\n",
            "\n",
            ">>>>validation loss: 3.3159::PPL 27.5480\n",
            "\n",
            ">>>>Training starts for epoch: 21\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 26.00\n",
            ">>>>train loss: 1.3363::PPL  3.8049\n",
            "\n",
            ">>>>validation loss: 3.3712::PPL 29.1128\n",
            "\n",
            ">>>>Training starts for epoch: 22\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 28.00\n",
            ">>>>train loss: 1.3077::PPL  3.6976\n",
            "\n",
            ">>>>validation loss: 3.4052::PPL 30.1212\n",
            "\n",
            ">>>>Training starts for epoch: 23\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 1.2679::PPL  3.5534\n",
            "\n",
            ">>>>validation loss: 3.4475::PPL 31.4217\n",
            "\n",
            ">>>>Training starts for epoch: 24\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 1.2370::PPL  3.4454\n",
            "\n",
            ">>>>validation loss: 3.4999::PPL 33.1114\n",
            "\n",
            ">>>>Training starts for epoch: 25\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 26.00\n",
            ">>>>train loss: 1.2097::PPL  3.3524\n",
            "\n",
            ">>>>validation loss: 3.5404::PPL 34.4804\n",
            "\n",
            ">>>>Training starts for epoch: 26\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 1.1812::PPL  3.2583\n",
            "\n",
            ">>>>validation loss: 3.5629::PPL 35.2667\n",
            "\n",
            ">>>>Training starts for epoch: 27\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 26.00\n",
            ">>>>train loss: 1.1553::PPL  3.1750\n",
            "\n",
            ">>>>validation loss: 3.6040::PPL 36.7432\n",
            "\n",
            ">>>>Training starts for epoch: 28\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 26.00\n",
            ">>>>train loss: 1.1282::PPL  3.0901\n",
            "\n",
            ">>>>validation loss: 3.6556::PPL 38.6926\n",
            "\n",
            ">>>>Training starts for epoch: 29\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 25.00\n",
            ">>>>train loss: 1.1072::PPL  3.0259\n",
            "\n",
            ">>>>validation loss: 3.6724::PPL 39.3477\n",
            "\n",
            ">>>>Training starts for epoch: 30\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 26.00\n",
            ">>>>train loss: 1.0890::PPL  2.9712\n",
            "\n",
            ">>>>validation loss: 3.6993::PPL 40.4178\n",
            "\n",
            ">>>>Training starts for epoch: 31\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 1.0664::PPL  2.9050\n",
            "\n",
            ">>>>validation loss: 3.7551::PPL 42.7402\n",
            "\n",
            ">>>>Training starts for epoch: 32\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 1.0519::PPL  2.8631\n",
            "\n",
            ">>>>validation loss: 3.7851::PPL 44.0408\n",
            "\n",
            ">>>>Training starts for epoch: 33\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 1.0362::PPL  2.8185\n",
            "\n",
            ">>>>validation loss: 3.8361::PPL 46.3428\n",
            "\n",
            ">>>>Training starts for epoch: 34\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 28.00\n",
            ">>>>train loss: 1.0231::PPL  2.7818\n",
            "\n",
            ">>>>validation loss: 3.8169::PPL 45.4635\n",
            "\n",
            ">>>>Training starts for epoch: 35\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 1.0069::PPL  2.7370\n",
            "\n",
            ">>>>validation loss: 3.8276::PPL 45.9517\n",
            "\n",
            ">>>>Training starts for epoch: 36\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 0.9958::PPL  2.7069\n",
            "\n",
            ">>>>validation loss: 3.9169::PPL 50.2462\n",
            "\n",
            ">>>>Training starts for epoch: 37\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 26.00\n",
            ">>>>train loss: 0.9821::PPL  2.6699\n",
            "\n",
            ">>>>validation loss: 3.9068::PPL 49.7396\n",
            "\n",
            ">>>>Training starts for epoch: 38\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 26.00\n",
            ">>>>train loss: 0.9655::PPL  2.6260\n",
            "\n",
            ">>>>validation loss: 3.9383::PPL 51.3331\n",
            "\n",
            ">>>>Training starts for epoch: 39\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 0.9549::PPL  2.5984\n",
            "\n",
            ">>>>validation loss: 3.9720::PPL 53.0884\n",
            "\n",
            ">>>>Training starts for epoch: 40\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 26.00\n",
            ">>>>train loss: 0.9462::PPL  2.5758\n",
            "\n",
            ">>>>validation loss: 3.9790::PPL 53.4656\n",
            "\n",
            ">>>>Training starts for epoch: 41\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 0.9358::PPL  2.5492\n",
            "\n",
            ">>>>validation loss: 4.0224::PPL 55.8370\n",
            "\n",
            ">>>>Training starts for epoch: 42\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 0.9277::PPL  2.5287\n",
            "\n",
            ">>>>validation loss: 4.0599::PPL 57.9695\n",
            "\n",
            ">>>>Training starts for epoch: 43\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 26.00\n",
            ">>>>train loss: 0.9184::PPL  2.5054\n",
            "\n",
            ">>>>validation loss: 4.0532::PPL 57.5826\n",
            "\n",
            ">>>>Training starts for epoch: 44\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 0.9094::PPL  2.4827\n",
            "\n",
            ">>>>validation loss: 4.0772::PPL 58.9822\n",
            "\n",
            ">>>>Training starts for epoch: 45\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 0.9027::PPL  2.4662\n",
            "\n",
            ">>>>validation loss: 4.1378::PPL 62.6650\n",
            "\n",
            ">>>>Training starts for epoch: 46\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 0.8988::PPL  2.4566\n",
            "\n",
            ">>>>validation loss: 4.1290::PPL 62.1148\n",
            "\n",
            ">>>>Training starts for epoch: 47\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 26.00\n",
            ">>>>train loss: 0.8897::PPL  2.4344\n",
            "\n",
            ">>>>validation loss: 4.1370::PPL 62.6167\n",
            "\n",
            ">>>>Training starts for epoch: 48\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 0.8869::PPL  2.4276\n",
            "\n",
            ">>>>validation loss: 4.1652::PPL 64.4069\n",
            "\n",
            ">>>>Training starts for epoch: 49\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 0.8807::PPL  2.4125\n",
            "\n",
            ">>>>validation loss: 4.1894::PPL 65.9805\n",
            "\n",
            ">>>>Training starts for epoch: 50\n",
            ">>>>Please wait and keep your screen active all the time.....\n",
            "\n",
            ">>>>time elapsed for this epoch is: 0: 01: 27.00\n",
            ">>>>train loss: 0.8758::PPL  2.4008\n",
            "\n",
            ">>>>validation loss: 4.2339::PPL 68.9885\n",
            "\n",
            ">>>>Training starts for epoch: 51\n",
            ">>>>Please wait and keep your screen active all the time.....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcYc9t5VL3Hi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}