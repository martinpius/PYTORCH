{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2AOgQulupoPJs/B3hOpH+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/PYTORCH/blob/main/MicroGradEngine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "t1 = timer()\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive/\", force_remount = True)\n",
        "  import random, torch, math\n",
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  print(f\">>>> You are in CoLaB with torch version: {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\">>>> {type(e)}: {e}\\n>>>> Please correct {type(e)} and reload\")\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\">>>> Available device: {device}\")\n",
        "def mytimer(t: float = timer())->float:\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"hrs: {h} mins: {m:>02} secs: {s:>05.2f}\"\n",
        "print(f\">>>> Time elapsed:\\t{mytimer(timer() - t1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjgQ2yuULv66",
        "outputId": "cd011707-de0e-41dd-cf14-b2e86ef20153"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            ">>>> You are in CoLaB with torch version: 2.0.1+cu118\n",
            ">>>> Available device: cpu\n",
            ">>>> Time elapsed:\thrs: 0 mins: 00 secs: 37.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We are implementing the Micrograd engine for backpropagation at elementary level.\n",
        "* This notebook reproduce the work of Andrej Karpathy: https://github.com/karpathy/micrograd much thanks for his great contribution\n"
      ],
      "metadata": {
        "id": "0kwyXOi2NYsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Micro:\n",
        "  \"\"\"\"\"\n",
        "  This class implement the micrograd engine\n",
        "  for carrying out BPP at elementary level\n",
        "  \"\"\"\"\"\n",
        "  def __init__(self,\n",
        "               data,\n",
        "               _children = (),\n",
        "               _op = \" \",\n",
        "               label = \" \"):\n",
        "    \n",
        "    self.grad = 0.0\n",
        "    self._backward = lambda: None\n",
        "    self.data = data\n",
        "    self._children = set(_children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  def __add__(self, other):\n",
        "    \"\"\"\"\"\n",
        "    This method implement the backpropagation\n",
        "    via an addition node\n",
        "    \"\"\"\"\"\n",
        "    other = other if isinstance(other, Micro) else Micro(other)\n",
        "    out = Micro(self.data + other.data, (self, other), \"+\", \"plus\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += 1.0 * out.grad\n",
        "      other.grad += 1.0 * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    \"\"\"\"\"\n",
        "    This method implement BPP through a multiplication node\n",
        "    \"\"\"\"\"\n",
        "    other = other if isinstance(other, Micro) else Micro(other)\n",
        "    out = Micro(self.data * other.data, (self, other), \"*\", \"mul\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += other.data * out.grad\n",
        "      other.grad += self.data * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __pow__(self, other):\n",
        "    \"\"\"\"\"\n",
        "    This method implement the BPP through a power node\n",
        "    \"\"\"\"\"\n",
        "    assert isinstance(other, (float, int))\n",
        "    out = Micro(self.data ** other, (self,), \"pow\", \"power\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += other * (self.data **(other - 1)) * out.grad\n",
        "    \n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def tanh(self):\n",
        "    \"\"\"\"\"\n",
        "    This method implement BPP through a tanh activation\n",
        "    \"\"\"\"\"\n",
        "    x = self.data\n",
        "    exp = math.exp(2 * x)\n",
        "    t = (exp - 1) / (exp + 1)\n",
        "    out = Micro(t, (self,), \"tanh\", \"activation\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (1 - t**2) * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  \n",
        "  def sigmoid(self):\n",
        "    \"\"\"\"\"\n",
        "    This method implement the BPP through a logistic activation\n",
        "    \"\"\"\"\"\n",
        "    x = self.data\n",
        "    exp = math.exp(x)\n",
        "    s = exp / (1 + exp)\n",
        "    out = Micro(s, (self,), \"sigmoid\", \"activation\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += s * (1 - s) * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def relu(self):\n",
        "    \"\"\"\"\"\n",
        "    This method implement the BPP through a ReLU\n",
        "    \"\"\"\"\"\n",
        "    x = self.data\n",
        "    r = 0 if self.data < 0 else self.data\n",
        "    out = Micro(r, (self,), \"ReLU\", \"activation\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (r > 0) * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "    \"\"\"\"\"\n",
        "    This method automate the BPP through the\n",
        "    entire network\n",
        "    \"\"\"\"\"\n",
        "    topo = [] # A collection of all nodes\n",
        "    visited = set() # To assess if the node is already attended\n",
        "    def build_topology(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._children:\n",
        "          build_topology(child)\n",
        "        topo.append(v)\n",
        "    build_topology(self)\n",
        "\n",
        "    self.grad = 1.0 # initialize the grad of the terminal node to 1 i.e dc/dc == 1\n",
        "    for node in reversed(topo):\n",
        "      node._backward()\n",
        "  \n",
        "  def __neg__(self):\n",
        "    return self * (-1)\n",
        "  \n",
        "  def __sub__(self, other):\n",
        "    return self + (-other)\n",
        "  \n",
        "  def __rsub__(self, other):\n",
        "    return other + (-self)\n",
        "  \n",
        "  def __rmul__(self, other):\n",
        "    return self * other\n",
        "  \n",
        "  def __truediv__(self, other):\n",
        "    return self * other ** (-1)\n",
        "  \n",
        "  def __rtruediv__(self, other):\n",
        "    return other * self **(-1)\n",
        "  \n",
        "  def __radd__(self, other):\n",
        "    return other + self\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return f\"Micro(data = {self.data}, grad = {self.grad})\"\n",
        "    \n"
      ],
      "metadata": {
        "id": "RDAu6MH7PeuL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing our micrograd function for a single neuron\n",
        "x1, x2, w1, w2 = Micro(2.0, label =\"x1\"), Micro(0.0, label = \"x2\"), Micro(-3.0, label = \"w1\"), Micro(4.0, label = \"w2\")\n",
        "x1w1 = w1 * x1 ; x1w1.label = \"x1w1\"\n",
        "x2w2 = w2 * x2 ; x2w2.label = \"x2w2\"\n",
        "x1w1x2w2 = x1w1 + x2w2 ; x1w1x2w2.label = \"x1w1x2w2\"\n",
        "b = Micro(6.78689948, label = \"bias\")\n",
        "n = x1w1x2w2 + b ; n.label = \"neuron\"\n",
        "o = n.tanh(); o.label = \"output\"\n",
        "o.backward()"
      ],
      "metadata": {
        "id": "8wlrFaQSY2ur"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert n.grad == x1w1x2w2.grad == b.grad == x1w1.grad == x2w2.grad"
      ],
      "metadata": {
        "id": "zNW1mxqFabTh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert w2.grad == 0.0"
      ],
      "metadata": {
        "id": "Uh30B9Qka41W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now we implement the simple neural network from its core\n"
      ],
      "metadata": {
        "id": "_LTAchDjbfMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Module:\n",
        "  \"\"\"\"\" The parent class to be inherited\n",
        "        by our models classes to reset the\n",
        "        gradients to zero before next step\n",
        "        of backpropagation\n",
        "  \"\"\"\"\"\n",
        "  def zero_grad(self):\n",
        "    for p in self.parameters():\n",
        "      p.grad = 0\n",
        "  \n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "class Neuron(Module):\n",
        "  \"\"\"\"\"\n",
        "  This class implement a neuron\n",
        "  --------------------\n",
        "  parameters:\n",
        "  n_in: input dim\n",
        "  \"\"\"\"\"\n",
        "  def __init__(self, n_in, act = True):\n",
        "    self.w = [Micro(random.uniform(-1,1)) for _ in range(n_in)] # initialize the weights\n",
        "    self.b = Micro(random.uniform(-1,1)) # initialize the bias\n",
        "    self.act = act\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    res = [wi*xi for wi, xi in zip(self.w, x)]\n",
        "    out = sum(res, self.b)\n",
        "    return out.relu() if self.act else out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.b] + self.w\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return f\"{'ReLU' if self.act else 'Linear'}Neuron({len(self.w)})\"\n",
        "\n",
        "class Layer(Module):\n",
        "  \"\"\"\"\"\n",
        "  This class implement a Layer\n",
        "  -----------------------------\n",
        "  parameters: n_in---> inputs dim, n_out ---> number of neurons\n",
        "  \"\"\"\"\"\n",
        "  def __init__(self, n_in, n_out, **kwargs):\n",
        "    self.neurons = [Neuron(n_in, **kwargs) for _ in range(n_out)]\n",
        "\n",
        "  def __call__(self, x):\n",
        "    outs = [n(x) for n in self.neurons]\n",
        "    return outs[0] if len(outs) == 1 else outs\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [p for n in self.neurons for p in n.parameters()]\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return f\"Layer of[{', '.join(str(n) for n in self.neurons)}]\"\n",
        "\n",
        "class MLP(Module):\n",
        "  \"\"\"\"\"\n",
        "  This class implement the MLP net\n",
        "  -------------------------\n",
        "  parameters\n",
        "  n_in : input dim n_out: output dim\n",
        "  \"\"\"\"\"\n",
        "  def __init__(self, n_in, n_out, **kwargs):\n",
        "    n_layers = [n_in] + n_out # list of all layers\n",
        "    self.layers = [Layer(n_layers[i], n_layers[i + 1], act = i != len(n_out) - 1) for i in range(len(n_out))]\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [p for l in self.layers for p in l.parameters()]\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return f\"MLP of[{', '.join(str(l) for l in self.layers)}]\"\n",
        "\n"
      ],
      "metadata": {
        "id": "6x49ks6FbzjA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing our model class:"
      ],
      "metadata": {
        "id": "M1NrxWpwjuSk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network with 5 layers\n",
        "n_in = 4\n",
        "n_out = [6, 9, 3, 1]\n",
        "mlp = MLP(n_in, n_out)\n",
        "\n",
        "# Create data for a simple binary problem\n",
        "samples = [[1.0, 2.0, 1.8, 2.0],\n",
        "        [3.8, 4.2, 2.8, 3.0],\n",
        "        [3.1, 2.8, 3.6, 1.0],\n",
        "        [1.1, 1.8, 2.7, 1.1],\n",
        "        [1.0, 1.4, 0.8, 1.6]]\n",
        "labs = [1, -1, -1, 1, 1]\n",
        "preds = [mlp(data) for data in samples]\n",
        "display(f\">>>> Predictions are:\")\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "_ARv1jSwj8S9",
        "outputId": "44767417-6f15-4d69-e23f-af5e4c91955f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'>>>> Predictions are:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Micro(data = 0.22956359855460248, grad = 0.0),\n",
              " Micro(data = 0.23261255990481733, grad = 0.0),\n",
              " Micro(data = 0.23901272972494236, grad = 0.0),\n",
              " Micro(data = 0.23404434337308302, grad = 0.0),\n",
              " Micro(data = 0.09064504437412427, grad = 0.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In the Next notebook we will train our network by using the Micrograd :-)"
      ],
      "metadata": {
        "id": "z1Ro1taQlmJj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QBZqSr-roL8p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}