{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNF3bxowIecR/K6zC9U5XQ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/PYTORCH/blob/main/MicroGradEngine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "t1 = timer()\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive/\", force_remount = True)\n",
        "  import torch, random, math\n",
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  print(f\">>>> You are on CoLaB with torch version: {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\">>>>{type(e)}: {e}\\n>>>> Please correct {type(e)} and reload\")\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\">>>> Available device: {device}\")\n",
        "def mytimer(t: float = timer())->float:\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"hrs: {h}, min: {m:>02}, sec: {s:>05.2f}\"\n",
        "print(f\">>>> Time elapsed: \\t{mytimer(timer() - t1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh0rYQBOTOOx",
        "outputId": "35989c64-4eac-4fc6-b0a1-8321d92fe8a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            ">>>> You are on CoLaB with torch version: 2.0.1+cu118\n",
            ">>>> Available device: cpu\n",
            ">>>> Time elapsed: \thrs: 0, min: 00, sec: 32.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Micro:\n",
        "\n",
        "  def __init__(self,\n",
        "               data,\n",
        "               _children = (),\n",
        "               _op = \" \",\n",
        "               label = \" \"):\n",
        "    self.grad = 0.0\n",
        "    self._backward = lambda: None\n",
        "    self.data = data\n",
        "    self._children = set(_children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  def __add__(self, other):\n",
        "    other = other if isinstance(other, Micro) else Micro(other)\n",
        "    out = Micro(self.data + other.data, (self, other), \"+\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += 1.0 * out.grad\n",
        "      other.grad += 1.0 * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "  \n",
        "  def __mul__(self, other):\n",
        "    other = other if isinstance(other, Micro) else Micro(other)\n",
        "    out = Micro(self.data * other.data, (self, other), \"*\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += other.data * out.grad\n",
        "      other.grad += self.data * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "  \n",
        "  def __pow__(self, other):\n",
        "    assert isinstance(other, (float, int))\n",
        "    out = Micro(self.data ** other, (self,), \"pow\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += other * (self.data ** (other - 1)) * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "  \n",
        "  def tanh(self):\n",
        "    x = self.data\n",
        "    exp = math.exp(2 * x)\n",
        "    t = (exp - 1) / (exp + 1)\n",
        "    out = Micro(t, (self,), \"tanh\")\n",
        "    \n",
        "    def _backward():\n",
        "      self.grad += (1 - t**2) * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "  \n",
        "  def sigmoid(self):\n",
        "    x = self.data\n",
        "    exp = math.exp(x)\n",
        "    s = exp / (1 + exp)\n",
        "    out = Micro(s, (self,), \"sigmoid\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += s * (1 - s) * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "  \n",
        "  def relu(self):\n",
        "    x = self.data\n",
        "    r = 0 if x < 0 else x\n",
        "    out = Micro(r, (self,), \"ReLU\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (r > 0) * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "  \n",
        "  def backward(self):\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topology(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._children:\n",
        "          build_topology(child)\n",
        "        topo.append(v)\n",
        "    build_topology(self)\n",
        "\n",
        "    self.grad = 1.0\n",
        "    for node in reversed(topo):\n",
        "      node._backward()\n",
        "  \n",
        "  def __radd__(self, other):\n",
        "    return self + other\n",
        "  \n",
        "  def __neg__(self):\n",
        "    return self * (-1)\n",
        "  \n",
        "  def __rmul__(self, other):\n",
        "    return  self * other\n",
        "  \n",
        "  def __sub__(self, other):\n",
        "    return self + (-other)\n",
        "  \n",
        "  def __rsub__(self, other):\n",
        "    return other + (-self)\n",
        "  \n",
        "  def __truediv__(self, other):\n",
        "    return self * (other **-1)\n",
        "  \n",
        "  def __rtruediv__(self, other):\n",
        "    return other * (self **-1)\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return f\"Micro(data = {self.data}, grad = {self.grad})\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-nRpaXceVOYN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1, x2 = Micro(2.0, label = \"x1\"), Micro(0.0, label = \"x1\")\n",
        "w1, w2 = Micro(-3.0, label = \"w1\"), Micro(4.0, label = \"w2\")\n",
        "x1w1 = w1 * x1 ; x1w1.label = \"x1w1\"\n",
        "x2w2 = w2 * x2 ; x2w2.label = \"x2w2\"\n",
        "x1w1x2w2 = x1w1 + x2w2 ; x1w1x2w2.label = \"x1w1x2w2\"\n",
        "b = Micro(6.8917736, label = \"bias\")\n",
        "n = x1w1x2w2 + b ; n.label = \"neuron\"\n",
        "o = n.tanh() ; o.label = \"output\"\n",
        "o.backward()"
      ],
      "metadata": {
        "id": "BZ7Rw28ee2pD"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "o.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDo0YaJkgE7e",
        "outputId": "553fd8d0-5f76-446d-93ae-2355206072cc"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert n.grad == x1w1x2w2.grad == x1w1.grad == x2w2.grad "
      ],
      "metadata": {
        "id": "QalGMGqOgSr5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert w2.grad == 0.0"
      ],
      "metadata": {
        "id": "5F0CHKUJgU6z"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Module:\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for p in self.parameters():\n",
        "      p.grad = 0\n",
        "  \n",
        "  def parameters(self):\n",
        "    return []\n"
      ],
      "metadata": {
        "id": "BTE5IvpegZLq"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron(Module):\n",
        "  def __init__(self, nin, non_lin = True):\n",
        "    self.w = [Micro(random.uniform(-1,1)) for _ in range(nin)]\n",
        "    self.b = Micro(random.uniform(-1,1))\n",
        "    self.non_lin = non_lin\n",
        "\n",
        "  def __call__(self, x):\n",
        "    out = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
        "    return out.relu() if self.non_lin else out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.b] + self.w\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return f\"{'ReLU' if self.non_lin else 'Linear'}Neuron({len(self.w)})\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Znq9dvnBgbKO"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(Module):\n",
        "  def __init__(self, nin, n_out, **kwargs):\n",
        "    self.neurons = [Neuron(nin, **kwargs) for _ in range(n_out)]\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    out = [n(x) for n in self.neurons]\n",
        "    return out[0] if len(out) == 1 else out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [p for n in self.neurons for p in n.parameters()]\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return f\"Layer of[{', '.join(str(n) for n in self.neurons)}]\"\n",
        "  "
      ],
      "metadata": {
        "id": "aDwquvcprLnj"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(Module):\n",
        "  def __init__(self, nin, n_out):\n",
        "    l_total = [nin] + n_out\n",
        "    self.layers = [Layer(l_total[i], l_total[i+1], non_lin = i != (len(n_out) - 1)) for i in range(len(n_out))]\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [p for l in self.layers for p in l.parameters()]\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return f\"MLP of [{', '.join(str(l) for l in self.layers)}]\""
      ],
      "metadata": {
        "id": "Hl5xBSYjs_lC"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 4\n",
        "inputs = [[2.0, 1.2, 3.1, 4.8],\n",
        "          [0.3, 0.8, 1.2, 0.8],\n",
        "          [2.9, 3.1, 4.0, 1.8],\n",
        "          [0.2, 1.3, 1.1, 1.4],\n",
        "          [3.1, 2.9, 3.4, 2.5]]\n",
        "targets = [1, -1, 1, -1, 1]\n",
        "\n",
        "n_out = [6, 8, 4, 1]\n",
        "\n",
        "model = MLP(input_dim, n_out)"
      ],
      "metadata": {
        "id": "KEmbxymmvV45"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = [model(x) for x in inputs]"
      ],
      "metadata": {
        "id": "aXeHOGuPBy-V"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBUbKDejB36m",
        "outputId": "3b4a87dc-2ef2-42b8-cec8-b1b710fbaea0"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Micro(data = -0.17896520509257785, grad = 0.0),\n",
              " Micro(data = -0.14181040660046146, grad = 0.0),\n",
              " Micro(data = -0.2898397306240814, grad = 0.0),\n",
              " Micro(data = -0.17896520509257785, grad = 0.0),\n",
              " Micro(data = -0.17896520509257785, grad = 0.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#[yi - pi for yi, pi in zip(targets, preds)]"
      ],
      "metadata": {
        "id": "DjTvULjJIaZ5"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "for epoch in range(10):\n",
        "  preds = [model(x) for x in inputs]\n",
        "  loss = sum((yi - pi)**2 for yi, pi in zip(targets, preds))\n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  for p in model.parameters():\n",
        "    p.data += -0.01 * p.grad\n",
        "  \n",
        "  print(epoch, loss)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "303vuWlCH5i0",
        "outputId": "7bf75541-618a-4231-c0f9-001175029e4c"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Micro(data = 0.20670489492110714, grad = 1.0)\n",
            "1 Micro(data = 0.2033048744659951, grad = 1.0)\n",
            "2 Micro(data = 0.15451289717291228, grad = 1.0)\n",
            "3 Micro(data = 0.19032124529359765, grad = 1.0)\n",
            "4 Micro(data = 0.2854279470244463, grad = 1.0)\n",
            "5 Micro(data = 0.29022781518606416, grad = 1.0)\n",
            "6 Micro(data = 0.19796330336247864, grad = 1.0)\n",
            "7 Micro(data = 0.19697178421946543, grad = 1.0)\n",
            "8 Micro(data = 0.14737930177339467, grad = 1.0)\n",
            "9 Micro(data = 0.18011713304748425, grad = 1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LEhvVfdXJ2u5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}