{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language Model: Text Generation in Pytorch using RNN with (LSTM) Architecture.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJ2cRUcQ+DszyBZAYdotrI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/PYTORCH/blob/main/Language_Model_Text_Generation_in_Pytorch_using_RNN_with_(LSTM)_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ak4UiEX3IZZ",
        "outputId": "3b19c150-82b2-40a9-9cdd-269dcca02a9a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import torch\n",
        "  print(f\">>>You are on Google CoLaB with Pytorch Version: {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\"{type(e)}: {e}\\n>>>Please correct {type(e)} and reload your device\")\n",
        "  COLAB = False\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "def time_fmt(t: float = 123.781)->float:\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"{h}: {m:>02}: {s:>05.2f}\"\n",
        "print(f\">>>time formating\\tplease wait...\\n>>>time elapsed\\t {time_fmt()}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            ">>>You are on Google CoLaB with Pytorch Version: 1.8.1+cu101\n",
            ">>>time formating\tplease wait...\n",
            ">>>time elapsed\t 0: 02: 03.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QV728P65Aop"
      },
      "source": [
        "#In this notebook we are going to train a character level language model to generate new texts\n",
        "#For demonstration we will apply a dataset with bunch of names to generate new names (Any text can be applied)\n",
        "#We are going to use rnn with an LSTM architecture "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfCWXBwu7euN",
        "outputId": "e7471c81-115c-4347-fdca-0e7a04ff7122"
      },
      "source": [
        "!pip install 'unidecode'"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "The folder you are executing pip from can no longer be found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqp1crqU6bzI"
      },
      "source": [
        "import torch, string\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time, sys,string, random\n",
        "import unidecode, os\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kumnpCg86szz"
      },
      "source": [
        "#We first read-in the text file which contain our data for this project:\n",
        "os.chdir(\"/content/drive/MyDrive/namesGen\")\n",
        "data_file = unidecode.unidecode(open(file = 'Names.txt').read())"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hn92wNEh-PGS",
        "outputId": "33da3b15-65a0-418e-86c7-a9327a6b4d8f"
      },
      "source": [
        "#print few lines of data.\n",
        "print(data_file[:297])\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aaban\n",
            "Aabharan\n",
            "Aabhas\n",
            "Aabhat\n",
            "Aabheer\n",
            "Aabheer\n",
            "Abheer\n",
            "Aabher\n",
            "Aabi\n",
            "Aabilesh\n",
            "Aabir\n",
            "Aabishan\n",
            "Aabishayan\n",
            "Aacharya\n",
            "Aachman\n",
            "Aachuthan\n",
            "Aadalalagan\n",
            "Aadalarasan\n",
            "Aadalazhagan\n",
            "Aadamya\n",
            "Aadanyan\n",
            "Aadarko\n",
            "Aadarsh\n",
            "Aadarshan\n",
            "Aadarshanan\n",
            "Aadav\n",
            "Aadavan\n",
            "Aadesh\n",
            "Aadesh\n",
            "Adesh\n",
            "Aadhan\n",
            "Aadhar\n",
            "Aadhav\n",
            "Aadhavan\n",
            "Aadhi\n",
            "Aadhiban\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1osIsaiW-Uwh"
      },
      "source": [
        "#Fetch reference characters from strings.prentable\n",
        "chars = string.printable\n",
        "num_chars = len(chars)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0nbaOleCwH1",
        "outputId": "fed8d77b-daef-41aa-b7d1-0a200f0500d3"
      },
      "source": [
        "print(f\"The reference characters are : {chars}\\nThe size of ref_char: {len(chars)}\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The reference characters are : 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
            "\r\u000b\f\n",
            "The size of ref_char: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWNRi1jBCytg"
      },
      "source": [
        "#We start by building our LSTM-class (The ussual LSTM):\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embed = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embed(x)\n",
        "        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n",
        "        out = self.fc(out.reshape(out.shape[0], -1))\n",
        "        return out, (hidden, cell)\n",
        "\n",
        "    def _initializer_(self, batch_size):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        return hidden, cell\n",
        "  \n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz-ksqgqTR3u"
      },
      "source": [
        "#We now build our generator class:\n",
        "class TextGen(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TextGen, self).__init__()\n",
        "    self.learning_rate = 1e-3\n",
        "    self.batch_size = 1\n",
        "    self.num_epochs = 2000\n",
        "    self.every_epoch = 50\n",
        "    self.text_chunk = 250\n",
        "    self.num_layers = 2\n",
        "    self.hidden_size = 256\n",
        "  \n",
        "  def _get_indices(self, string):\n",
        "    '''This method build a tensor of indices from string'''\n",
        "    #place holder for the indeces\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    #Fetch the index for every char from the reference list and store in tensor\n",
        "    for c in range(len(string)):\n",
        "      tensor[c] = chars.index(string[c])\n",
        "    return tensor\n",
        "  \n",
        "  def _rndbatch_(self):\n",
        "    '''select random chunk of texts from the data file'''\n",
        "    start_idx = random.randint(0, len(data_file) - self.text_chunk)\n",
        "    end_idx = start_idx + self.text_chunk + 1\n",
        "    text_str = data_file[start_idx:end_idx]\n",
        "    #place holder for the input and output data\n",
        "    input_text = torch.zeros(self.batch_size, self.text_chunk)\n",
        "    target_text = torch.zeros(self.batch_size, self.text_chunk)\n",
        "    for i in range(self.batch_size):\n",
        "      input_text[i,:] = self._get_indices(text_str[:-1])\n",
        "      target_text[i,:] = self._get_indices(text_str[1:]) #We offset by 1 to predict next char\n",
        "    return input_text.long(), target_text.long()\n",
        "  \n",
        "  def _generate_(self, initial_str = 'Aa', pred_len = 100, temperature = 0.85):\n",
        "    h0,c0 = self.rnn._initializer_(batch_size = self.batch_size)\n",
        "    initial_input = self._get_indices(initial_str)\n",
        "    predicted = initial_str #first string always predicted to the same\n",
        "    for p in range(len(initial_str) - 1):\n",
        "      _, (hidden,cell) = self.rnn(\n",
        "          initial_input[p].view(1).to(device), c0,h0)\n",
        "    last_char = initial_input[-1]\n",
        "\n",
        "    for p in range(pred_len):\n",
        "      out, (hidden, cell) = self.rnn(last_char.view(1).to(device), h0, c0)\n",
        "      out_dstn = out.data.view(-1).div(temperature).exp()\n",
        "      top_chars = torch.multinomial(out_dstn,1)[0]\n",
        "      predicted_char = chars[top_chars]\n",
        "      predicted+=predicted_char\n",
        "      last_char = self._get_indices(predicted_char)\n",
        "    return predicted\n",
        "  \n",
        "  #Now we trin our model (through the following train function)\n",
        "  def train(self):\n",
        "    self.rnn = LSTM(num_chars, self.hidden_size, self.num_layers,num_chars).to(device)\n",
        "    optimizer = optim.Adam(params = self.rnn.parameters(), lr = self.learning_rate)\n",
        "    loss_obj = nn.CrossEntropyLoss()\n",
        "    writer = SummaryWriter(f\"runs/names0\")\n",
        "    print(f\"\\n>>>training starts please wait....\")\n",
        "    for epoch in range(1, self.num_epochs + 1):\n",
        "      data, target = self._rndbatch_()\n",
        "      h0,c0 = self.rnn._initializer_(self.batch_size)\n",
        "\n",
        "      self.rnn.zero_grad()\n",
        "      loss = 0\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      for c in range(self.text_chunk):\n",
        "        output, (hidden, cell) = self.rnn(data[:,c], h0,c0)\n",
        "        loss+=loss_obj(output, target[:,c])\n",
        "      \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss = loss.item()/self.text_chunk\n",
        "\n",
        "      if epoch % self.every_epoch == 0:\n",
        "        print(f\"\\n>>>>Loss is: {loss:.4f}\")\n",
        "        print(f\"\\n>>>>{self._generate_()}\")\n",
        "      writer.add_scalar(\"Training loss\", loss, global_step = epoch)\n",
        "  "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vL7zDVn2sMa"
      },
      "source": [
        "#Running the model to generate new names\n",
        "names_gen = TextGen()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCaIA5B_3jWJ",
        "outputId": "4e9fe07e-4256-41a5-9645-d7944e69db4f"
      },
      "source": [
        "tic = time.time()\n",
        "names_gen.train()\n",
        "toc = time.time()\n",
        "print(f\"\\n>>>>time elapsed: {time_fmt(toc - tic)}\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>training starts please wait....\n",
            "\n",
            ">>>>Loss is: 2.8447\n",
            "\n",
            ">>>>Aan\n",
            "enyi\n",
            "QAjee\n",
            "Kanakarirayajalva\n",
            "jalinteujavavai\n",
            "thathan\n",
            "+Q`\n",
            "rasha\n",
            "ffdhi\n",
            "ha\n",
            "6|z9Rath\n",
            "0nran\n",
            "Sharal\n",
            "Vimh\n",
            "\n",
            ">>>>Loss is: 2.4105\n",
            "\n",
            ">>>>Aabukurinubar\n",
            "Atha\n",
            "Knryananminthashava\n",
            "Har\n",
            "Tha\n",
            "Dha\n",
            "Sajidran\n",
            "Anar\n",
            "Raonimethasotrathathaeyurilirajishava\n",
            "\n",
            ">>>>Loss is: 2.2956\n",
            "\n",
            ">>>>Aa\n",
            "Kener\n",
            "Vearikaleei\n",
            "Su\n",
            "Dama\n",
            "Keerajorannanineakarajana\n",
            "Kanerathinthoparagadea\n",
            "Mera\n",
            "Ran\n",
            "Ke\n",
            "Mhathath\n",
            "Ya\n",
            "\n",
            "\n",
            ">>>>Loss is: 2.6865\n",
            "\n",
            ">>>>Aamanavishanith\n",
            "Vo\n",
            "Kininugeysho+niI]|5anarishitashajothrin\n",
            "Sajalrinanta\n",
            "Jithrithari\n",
            "Noraruyaveshn\n",
            "Kish\n",
            "\n",
            ">>>>Loss is: 2.4908\n",
            "\n",
            ">>>>Aakubil\n",
            "Naleygan\n",
            "Han\n",
            "UhukanevoPakalarApin\n",
            "Danubhraminthi\n",
            "Dhaganunavaranan\n",
            "Abharthan\n",
            "Dham\n",
            "Gartukuravi\n",
            "A\n",
            "\n",
            ">>>>Loss is: 2.9119\n",
            "\n",
            ">>>>Aarilandrekash\n",
            "Paan\n",
            "Jeshdhra\n",
            "An\n",
            "Panvin\n",
            "Van\n",
            "Vthan\n",
            "Pa\n",
            "Bakayalthatathljmalanal\n",
            "Jan\n",
            "Ashareshithanythimir\n",
            "D\n",
            "\n",
            ">>>>Loss is: 2.3865\n",
            "\n",
            ">>>>Aani\n",
            "Asha\n",
            "Mangashikathan\n",
            "So\n",
            "Rasrnuta\n",
            "Vizigema\n",
            "Thiyanashin\n",
            "Avara\n",
            "Handhakedhika\n",
            "Ni\n",
            "T\n",
            "Vra\n",
            "Ptha\n",
            "Ra\n",
            "Kaana\n",
            "S\n",
            "\n",
            ">>>>Loss is: 2.2878\n",
            "\n",
            ">>>>Aar\n",
            "Khit\n",
            "banibhanganandashanyali\n",
            "Vi\n",
            "Veshineesranishatharishanarithiajadana\n",
            "Vithu\n",
            "Veeemamanikan\n",
            "Ka\n",
            "Jan\n",
            "\n",
            "\n",
            ">>>>Loss is: 2.1059\n",
            "\n",
            ">>>>Aavarishiksh\n",
            "Tanjalyan\n",
            "Raan\n",
            "Ban\n",
            "Amik\n",
            "Pa\n",
            "Canaravivathith\n",
            "Rathivan\n",
            "Melthath\n",
            "Rishvanivinthshitha\n",
            "un\n",
            "Kaba\n",
            "\n",
            "\n",
            ">>>>Loss is: 2.2961\n",
            "\n",
            ">>>>Aanthanthathin\n",
            "Na\n",
            "Adatashan\n",
            " Kva\n",
            "Jeyana\n",
            "Rathyanan\n",
            "Thithar\n",
            "Di\n",
            "Ran\n",
            "Bokantaga\n",
            "Hishlaragikukandhikakash\n",
            "Sh\n",
            "\n",
            ">>>>Loss is: 2.0550\n",
            "\n",
            ">>>>Aaban\n",
            "Halaaana\n",
            "Ka\n",
            "San\n",
            "Ka\n",
            "Kandnash\n",
            "inam\n",
            "Jesha\n",
            "Rale\n",
            "Gathanshalshararini\n",
            "Ke\n",
            "Sa\n",
            "Saaaya\n",
            "Kipain\n",
            "Chagara\n",
            "Jesh\n",
            "\n",
            ">>>>Loss is: 2.4128\n",
            "\n",
            ">>>>Aanan\n",
            "Repuna\n",
            "Ashanadaanani\n",
            "Ran\n",
            "Kiyeevita\n",
            "Siniyan\n",
            "Ruythujayananakaji\n",
            "Anunthra thi\n",
            "Aenithri\n",
            "Dameesha\n",
            "Van\n",
            "\n",
            ">>>>Loss is: 2.1494\n",
            "\n",
            ">>>>Aaghani\n",
            "Dha\n",
            "Egalanlumosv\n",
            "Thikrilanavipu\n",
            "Sr\n",
            "Gishanjaksi\n",
            "Vaaukeshathsheyan\n",
            "Ul\n",
            "Uluvanalrshakani\n",
            "Ukralala\n",
            "\n",
            "\n",
            ">>>>Loss is: 2.4312\n",
            "\n",
            ">>>>Aa\n",
            "Nish\n",
            "Nith\n",
            "Th\n",
            "Ka\n",
            "Athrun\n",
            "Th\n",
            "Meni\n",
            "Jeva\n",
            "Anishyan\n",
            "Thalirath\n",
            "Aani\n",
            "Ahidhith\n",
            "Adatha\n",
            "Thunaivishrathaanithan\n",
            "\n",
            "\n",
            ">>>>Loss is: 2.2034\n",
            "\n",
            ">>>>Aan\n",
            "Jararuluanarur\n",
            "Amidn\n",
            "Yashan\n",
            "Ranysharavan\n",
            "Ca\n",
            "Abanthanirsh\n",
            "Shikshyamma\n",
            "Bathamwarirama\n",
            "Pashaden\n",
            "Abesh\n",
            "\n",
            ">>>>Loss is: 2.1664\n",
            "\n",
            ">>>>Aa\n",
            "Anoothimashabaaka\n",
            "Ki\n",
            "Bamy\n",
            "Patis\n",
            "Sishyai\n",
            "Mi\n",
            "Miladeshina\n",
            "Randru\n",
            "Di\n",
            "Arvaml\n",
            "Asa\n",
            "Jaganalin\n",
            "Morulishee\n",
            "As\n",
            "\n",
            ">>>>Loss is: 2.2119\n",
            "\n",
            ">>>>Aanatakamaku\n",
            "Sa\n",
            "Ri\n",
            "Moshan\n",
            "Du\n",
            "Narthan\n",
            "U\n",
            "Bandanin\n",
            "Trathakak\n",
            "Hashinagya\n",
            "Haminaduda\n",
            "Bianantarthala\n",
            "Hash\n",
            "Ya\n",
            "\n",
            ">>>>Loss is: 2.4950\n",
            "\n",
            ">>>>Aakrm\n",
            "Khan\n",
            "Kahinulalarthaharum\n",
            "Jalathikshani\n",
            "Tunish\n",
            "Nasharanyani\n",
            "Laylinokinidrun\n",
            "Samarmyanithithilan\n",
            "P\n",
            "\n",
            ">>>>Loss is: 2.3717\n",
            "\n",
            ">>>>Aavithasha\n",
            "Na\n",
            "Kee\n",
            "Momaniulayalil\n",
            "Ka\n",
            "Mowahaya\n",
            "Ga\n",
            "Vikaraivan\n",
            "Husha\n",
            "Abithaluveyafbhasalr\n",
            "Hikan\n",
            "Liyak\n",
            "Imis\n",
            "\n",
            ">>>>Loss is: 1.9444\n",
            "\n",
            ">>>>Aahisheka\n",
            "Baa\n",
            "Vithithan\n",
            "Mari\n",
            "Vina\n",
            "Nuri\n",
            "Shar\n",
            "Putemithiputhetherin\n",
            "Nisan\n",
            "Baswsh\n",
            "Nikalketan\n",
            "Nalasan\n",
            "Ii\n",
            "Am\n",
            "\n",
            ">>>>Loss is: 2.4478\n",
            "\n",
            ">>>>Aa\n",
            "Ami\n",
            "Pra\n",
            "Gen\n",
            "Aji\n",
            "Hakshrila\n",
            "Shethai\n",
            "Koja\n",
            "Esha\n",
            "Kar\n",
            "Pa\n",
            "Mor\n",
            "Dhlikariya\n",
            "Vinetyga\n",
            "Ari\n",
            "Ka\n",
            "Java\n",
            "Ashin\n",
            "Sa\n",
            "Amk\n",
            "\n",
            ">>>>Loss is: 2.1777\n",
            "\n",
            ">>>>Aabdra\n",
            "Puaka\n",
            "Ashanjani\n",
            "Vi\n",
            "Van\n",
            "Sarivan\n",
            "Rutharstitheen\n",
            "V\n",
            "Enaathavyan\n",
            "Leeshalodeya\n",
            "Aanaran\n",
            "Meenthi\n",
            "Chee\n",
            "G\n",
            "\n",
            ">>>>Loss is: 2.0902\n",
            "\n",
            ">>>>Aan\n",
            "Ash\n",
            "Ra\n",
            "Akhanti\n",
            "Deelash\n",
            "Pai\n",
            "Odesuka\n",
            "Itha\n",
            "Kanumi\n",
            "Ealaelamilanabani\n",
            "Devin\n",
            "Varulan\n",
            "Ka\n",
            "Athashanan\n",
            "Kar\n",
            "P\n",
            "\n",
            ">>>>Loss is: 2.3543\n",
            "\n",
            ">>>>Aan\n",
            "Ikevaravi\n",
            "Vey\n",
            "Thari\n",
            "Dhan\n",
            "Am\n",
            "Prinitha\n",
            "Valan\n",
            "Aaran\n",
            "San\n",
            "Ch\n",
            "Rojapanirijit\n",
            "Harikha\n",
            "Lara\n",
            "Ranthinga\n",
            "Neni\n",
            "\n",
            "\n",
            ">>>>Loss is: 2.1573\n",
            "\n",
            ">>>>Aaniy\n",
            "Shabhmita\n",
            "Thn\n",
            "Sharmaka\n",
            "Nishav\n",
            "Bh\n",
            "Jan\n",
            "Kukarthana\n",
            "Sa\n",
            "Thi\n",
            "Aranty\n",
            "Soiyan\n",
            "Renaeethuraini\n",
            "Tharanikasra\n",
            "\n",
            ">>>>Loss is: 1.8144\n",
            "\n",
            ">>>>Aaminchunan\n",
            "Sh\n",
            "Pra\n",
            "Am\n",
            "Arara\n",
            "Egaya\n",
            "Jeshyathvevira\n",
            "Kaghari\n",
            "Krshathuthi\n",
            "Vith\n",
            "Vedhi\n",
            "Ri\n",
            "Nelan\n",
            "Thumela\n",
            "Shish\n",
            "\n",
            ">>>>Loss is: 2.5090\n",
            "\n",
            ">>>>Aa\n",
            "Siyan\n",
            "Tiranyan\n",
            "Meth\n",
            "Deeranundulithartha\n",
            "Ain\n",
            "Thylatha\n",
            "Nishinithuman\n",
            "Vilan\n",
            "Sishanthanthaksh\n",
            "Dhwaasamc\n",
            "\n",
            ">>>>Loss is: 2.4404\n",
            "\n",
            ">>>>Aakayamitarahijan\n",
            "Rumaruga\n",
            "Vararethi\n",
            "Yudhal\n",
            "Runajarkshanarutharai\n",
            "Adrushikrin\n",
            "Mu\n",
            "Nan\n",
            "Prareedhumanin\n",
            "Ja\n",
            "\n",
            ">>>>Loss is: 1.9515\n",
            "\n",
            ">>>>Aarishikshaninowaruni\n",
            "Dushanchashan\n",
            "Ah\n",
            "Pan\n",
            "Ni\n",
            "Diskhanarthan\n",
            "Pananutha\n",
            "Ilath\n",
            "San\n",
            "Arn\n",
            "Hipanya\n",
            "Pin\n",
            "Kanire\n",
            "\n",
            ">>>>Loss is: 1.7896\n",
            "\n",
            ">>>>Aanthani\n",
            "Daga\n",
            "Koopothmamar\n",
            "Mulanthathusha Si\n",
            "Va\n",
            "Yaniyanmanje\n",
            "Mavantha\n",
            "Kaadeshaami\n",
            "Kaneeeshanjeena\n",
            "Aran\n",
            "\n",
            ">>>>Loss is: 2.2873\n",
            "\n",
            ">>>>Aashavasheesv\n",
            "Dadosusharyanan\n",
            "Pavahvin\n",
            "Vazhaniyavasharurik\n",
            "Anulathan\n",
            "Tataniethan\n",
            "larisha\n",
            "Rashadheh\n",
            "Jan\n",
            "\n",
            ">>>>Loss is: 2.1372\n",
            "\n",
            ">>>>Aa\n",
            "Shaaya\n",
            "Kada\n",
            "Kann\n",
            "Ni\n",
            "Ven\n",
            "Vi\n",
            "Revaa\n",
            "Mala\n",
            "Mi\n",
            "Yan\n",
            "Shishin\n",
            "Chiyarirish\n",
            "Bha\n",
            "Peraashiri\n",
            "Ma\n",
            "Ra\n",
            "Nayaman\n",
            "Dhash\n",
            "\n",
            ">>>>Loss is: 2.4719\n",
            "\n",
            ">>>>Aanga\n",
            "Kavkaivishanan\n",
            "Nin\n",
            "Vanikhan\n",
            "Man\n",
            "Ananilelan\n",
            "Shishnaama\n",
            "Nishan\n",
            "Mooothanahikamika\n",
            "Asanira\n",
            "Jashi\n",
            "Pr\n",
            "\n",
            "\n",
            ">>>>Loss is: 2.1008\n",
            "\n",
            ">>>>Aan\n",
            "Than\n",
            "Gan\n",
            "Vinnithanyanthan\n",
            "Sadh Ba\n",
            "Nunikarushananajerrshagivi\n",
            "Praasharin\n",
            "Sarsharngraarthamanararant\n",
            "\n",
            ">>>>Loss is: 2.4442\n",
            "\n",
            ">>>>Aash\n",
            "Aayu\n",
            "Hemishen\n",
            "Sala\n",
            "Poth\n",
            "Amesharishat\n",
            "Kalvan\n",
            "Bandeehthrirmadelnaguvaleraviranan\n",
            "Ka\n",
            "Sa\n",
            "Shan\n",
            "Dhalaja\n",
            "\n",
            ">>>>Loss is: 1.9023\n",
            "\n",
            ">>>>Aasadh\n",
            "V\n",
            "Kra\n",
            "Bha\n",
            "Snathanatha\n",
            "Charn\n",
            "Rajyishalvath\n",
            "Vatagevagiyanthashenal\n",
            "Kr\n",
            "Hev\n",
            "Lar\n",
            "Dhelemaprukkubhansh\n",
            "\n",
            ">>>>Loss is: 2.2599\n",
            "\n",
            ">>>>Aajaneeshagesitabula\n",
            "Jeevadgara\n",
            "Gvan\n",
            "Iyaa\n",
            "Sa\n",
            "Irin\n",
            "Shaana\n",
            "Neyakivinshekandh\n",
            "Arnarukarin\n",
            "Ati\n",
            "Maja\n",
            "Ara\n",
            "Ar\n",
            "\n",
            ">>>>Loss is: 2.2865\n",
            "\n",
            ">>>>Aandh\n",
            "Juthan\n",
            "An\n",
            "Panaicatharitha\n",
            "Vanasava\n",
            "Prishushikayaandrila\n",
            "Man\n",
            "Min\n",
            "Kararmishin\n",
            "Thane\n",
            "Kittha\n",
            "Neshari\n",
            "\n",
            ">>>>Loss is: 2.4067\n",
            "\n",
            ">>>>Aan\n",
            "Deshaaan\n",
            "Sutajarindraranre\n",
            "Hacha\n",
            "P\n",
            "San\n",
            "Neyusishindhadhath\n",
            "Ja\n",
            "Meen\n",
            "Shorubajaankajar\n",
            "Dashai\n",
            "In\n",
            "Chesr\n",
            "\n",
            ">>>>Loss is: 2.4271\n",
            "\n",
            ">>>>Aanarma\n",
            "Rulanashman\n",
            "Aran\n",
            "Sumaran\n",
            "Kooooganan\n",
            "Vanar\n",
            "Aararan\n",
            "Chi\n",
            "Li\n",
            "Amajan\n",
            "Abhan\n",
            "Aavevi\n",
            "Abandr\n",
            "Galeen\n",
            "Sha\n",
            "\n",
            ">>>>time elapsed: 0: 07: 54.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VbRBIHU4I_g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}