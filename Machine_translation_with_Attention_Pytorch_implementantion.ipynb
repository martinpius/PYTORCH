{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine translation with Attention: Pytorch implementantion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNipO8XeTLjWAxfAVBM6ytc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/PYTORCH/blob/main/Machine_translation_with_Attention_Pytorch_implementantion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2zbU6Kr3Sji",
        "outputId": "db6f7497-a0cb-46ea-a4c2-36ee085c426f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(f\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import torch \n",
        "  print(f\"You are on CoLaB with torch version: {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\"{type(e)}: {e}\\n>>>>>please correct {type(e)} and re-load your device\")\n",
        "  COLAB = False\n",
        "def time_fmt(t: float = 231.718)->float:\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"{h}: {m:>02}: {s:>05.2f}\"\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print(f\">>>>>time testing:\\tplease wait....\\n>>>>time elapsed:\\t{time_fmt()}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "You are on CoLaB with torch version: 1.8.1+cu101\n",
            ">>>>>time testing:\tplease wait....\n",
            ">>>>time elapsed:\t0: 03: 51.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuGqOH0T45qN"
      },
      "source": [
        "#In this notebook we are going to train a neural machine translation using RNN\n",
        "#The model architecture involves attention mechanism.\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2TX8njz6FPz"
      },
      "source": [
        "#The dataset to be used for this model comes from torchtext (Multi30k) class:\n",
        "import torch, random, time, spacy, sys, math\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDRH1frj-lE8"
      },
      "source": [
        "#Set the device to avoid cuda errors:\n",
        "seed = 2324\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8tLyJFJ_S21",
        "outputId": "8c2ec5a0-0fe7-490c-8190-e66b03d26e3f"
      },
      "source": [
        "#Installing and loading the tokenizers for data pre-processing\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "#Loading the tokenizers\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "spacy_de = spacy.load('de_core_news_sm') #restart the kernel for effective loading"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (56.0.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (56.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.10.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKLjM53jAEG5"
      },
      "source": [
        "#Functions to perform tokenization with the aid of the tokenizers above and we also\n",
        "#reversing the sentences for the case of germany language to improve generalization\n",
        "def en_tokenizer(text):\n",
        "  return  [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "def de_tokenizer(text):\n",
        "  return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmUCWj-2BKmK"
      },
      "source": [
        "#Apply the above function to preprocess the data by tokenize them, \n",
        "#adding start and end of the sentence,and lower cases each token\n",
        "germany_ln = Field(tokenize = de_tokenizer, lower = True, init_token = '<sos>', eos_token = '<eos>',include_lengths = True)\n",
        "english_ln = Field(tokenize = en_tokenizer, lower = True, init_token = '<sos>', eos_token = '<eos>')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u737Fr1PCa0o",
        "outputId": "52a4ec5d-432f-4fce-d996-6f20b6c4f547"
      },
      "source": [
        "#We can now load and preprocess our data using Multi30k class:\n",
        "#The download and preprocessing may take a while depend with internet and computing power:\n",
        "tic = time.time()\n",
        "print(f\">>>>Please wait while the data is downloaded to CoLaB......\")\n",
        "train_data, validation_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (germany_ln, english_ln))\n",
        "toc = time.time()\n",
        "print(f\">>>>Time elapsed for downloading and preprocessing the data is: {time_fmt(toc - tic)}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>>Please wait while the data is downloaded to CoLaB......\n",
            ">>>>Time elapsed for downloading and preprocessing the data is: 0: 24: 46.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-fgvqdJEtk4",
        "outputId": "d3f7da86-c70c-4d40-8749-c8c54e90bf37"
      },
      "source": [
        "#We can also print number of examples in each dataset to see if we have downloaded the correct files\n",
        "print(f\"num_train_examples: {len(train_data.examples)}\\nnum_valid_examples: {len(validation_data.examples)}\\nnum_test_examples: {len(test_data.examples)}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_train_examples: 29000\n",
            "num_valid_examples: 1014\n",
            "num_test_examples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1iAHQ3YE69w",
        "outputId": "b07751bb-b5cd-4609-e2ae-d295b5c1916a"
      },
      "source": [
        "#We can also investigate a single data example to see if the sentences are well tokenized:\n",
        "print(f\"sample token list: {vars(train_data.examples[0])}\") #The first example in our training data"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample token list: {'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSWF9zhWF3S5"
      },
      "source": [
        "#We build-up vocabulary list for our training data only to avoid falacy in validation and test set\n",
        "germany_ln.build_vocab(train_data, min_freq = 2) #We take those tokens which repeated for at least 2 times\n",
        "english_ln.build_vocab(train_data, min_freq = 2) "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itz-nYvAGibR"
      },
      "source": [
        "#We can finally build our iterator that will be streamed into our model later during training-validation stage\n",
        "batch_size = 64\n",
        "train_iter, validation_iter, test_iter = BucketIterator.splits(\n",
        "    (train_data, validation_data, test_data),\n",
        "    batch_size = batch_size,\n",
        "    sort_within_batch = True,\n",
        "    sort_key = lambda x: len(x.src),\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRFgDDmrIniN"
      },
      "source": [
        "#Now we are good to go for the model Building. The Encoder is a ussual rnn with GRU architecture \n",
        "#With modification in the call method to force the model to accept information from both tokens \n",
        "#and length of each sentences"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ria9DIZKIuY5"
      },
      "source": [
        "#This class will use one Bi-directional GRU layer for encoder: Note that shape for output of encoder\n",
        "#Is the same as hidden layer size for decoder\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, embd_dim, enc_hidden, dec_hidden, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(input_dim, embd_dim)\n",
        "    self.rnn = nn.GRU(embd_dim, enc_hidden, bidirectional = True)\n",
        "    self.fc =  nn.Linear(2*enc_hidden, dec_hidden)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input, input_len):\n",
        "    '''\n",
        "    Note: input dim = [input_len = germany_vocab_len, batch_size]\n",
        "    input_len dim = [batch_size]\n",
        "    '''\n",
        "    embedded = self.dropout(self.embedding(input)) # embedded shape = [input_len, batch_size, embd_dim]\n",
        "    #rnn.pack_padded_seq is used to produce pack-padded sequence of the padded imbedded sequence\n",
        "    packed_embd = nn.utils.rnn.pack_padded_sequence(embedded, input_len.to('cpu'))#We exclusively store sentences length to cpu\n",
        "    #The packed output = packed sequence that consists of all hidden states\n",
        "    packed_output, hidden = self.rnn(packed_embd) #Here hidden is from non-padded sequence of the batch\n",
        "    #we then run rnn_pad_packed_seq to with the packed input to  unpack \n",
        "    #Outputs are unpacked states where the padded inputs are now not considered in computation (all zeros)\n",
        "    output, _ = nn.utils.rnn.pad_packed_sequence(packed_output) #shape for the output [seq_len, batch, hidden*2(bidirectional)]\n",
        "    #We neeed to concatenate output of both forwrd direction and backward direction rnn hidden states\n",
        "    #for the last element of backward direction rnn = [-1,:,:] and for the forward direction rnn: [-2,:,:]\n",
        "    #Initial decoder hidden size correspond to the output of linear layer that use the concatenated hidden layers above\n",
        "    hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "    #output shape = [input_len, batch_size, 2*hidden], hidden_dim = [batch_size, dec_hidden_dim]\n",
        "    return output, hidden\n"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UacFbebN0mQS"
      },
      "source": [
        "#We now build the attention mechanism to be utilized by our network\n",
        "#In this case we do not pay attention to the padded indices (zeros)\n",
        "#We force the attention to be over only real tokens (ignore padding effect) \n",
        "#using a mask. We supply a mask of shape [batch_size, len_sentence] \n",
        "#which take 1 if no padded and zero else where is supplied as additional\n",
        "#details to the foward method of our attention class.\n"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YSlyzlFMT0G"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hidden_size, dec_hidden_size):\n",
        "    super().__init__()\n",
        "    self.attn = nn.Linear(enc_hidden_size*2 + dec_hidden_size, dec_hidden_size)\n",
        "    self.v = nn.Linear(dec_hidden_size, 1, bias = False)\n",
        "  \n",
        "  def forward(self, dec_hidden,enc_outputs, mask):\n",
        "    '''\n",
        "    Note that: dec_hidden_dim = [batch_size, dec_hidden_dim]\n",
        "    enc_outputs_dim = [input_len, batch_size, enc_hidden*2]\n",
        "    '''\n",
        "    batch_size = enc_outputs.shape[1]\n",
        "    input_len = enc_outputs.shape[0]\n",
        "    #Since decoder prev is merged with enc_hidden we have to repeat it times the len_input to equalize the shape\n",
        "    dec_hidden = dec_hidden.unsqueeze(1).repeat(1, input_len,1)\n",
        "    enc_outputs = enc_outputs.permute(1,0,2) #to allow dot-product we interchange first and 2nd dim for the encoder out\n",
        "    #enc_outputs_shape: [batch_size, input_len, enc_hidden*2], dec_hidden_shape = [batch_size, input_len, dec_hidden_dim]\n",
        "    e_values = torch.tanh(self.attn(torch.cat((dec_hidden, enc_outputs),dim = 2))) #shape = [batch_size, input_len, dec_hidden]\n",
        "    attention = self.v(e_values).squeeze(2) #Shape = [batch_size, input_len]\n",
        "    #Now we apply the mask to the attention before the softmax transformation\n",
        "    attention = attention.masked_fill(mask == 0, 1e-10)\n",
        "    return nn.functional.softmax(attention, dim = 1) #accros the input_len dimension (the sentence entered)\n"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk8ydOApW9UK"
      },
      "source": [
        "#The decoder Network is the typical rnn with GRU architecture but\n",
        "#We write the class in such a way that it will accept the mask over the input_sentence and pass\n",
        "#it to the attention class to compute by ignore the indices for the padded tokens\n",
        "#We also returns the attention tensor for vissualization later during inference\n"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T5u41LWXjjL"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "    super().__init__()\n",
        "    self.output_dim = output_dim\n",
        "    self.attention = attention\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "    self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, input, hidden, encoder_outputs, mask):\n",
        "    input = input.unsqueeze(0)\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    a = self.attention(hidden, encoder_outputs, mask)\n",
        "    a = a.unsqueeze(1)\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "    weighted = torch.bmm(a, encoder_outputs)\n",
        "    weighted = weighted.permute(1, 0, 2)\n",
        "    rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "    output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "    assert (output == hidden).all()\n",
        "    embedded = embedded.squeeze(0)\n",
        "    output = output.squeeze(0)\n",
        "    weighted = weighted.squeeze(0)\n",
        "    prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "    return prediction, hidden.squeeze(0), a.squeeze(1)"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8FzBMGkMQQE"
      },
      "source": [
        "#We now combine the above classes (encoder , decoder) to build our seq2seq model:\n",
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self, encoder, decoder,input_pad_idx, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_pad_idx = input_pad_idx\n",
        "    self.device = device\n",
        "  \n",
        "  def mask_build(self, input):\n",
        "    '''\n",
        "    mask will have 1 if no padded index and 0 otherwise\n",
        "    '''\n",
        "    mask = (input != self.input_pad_idx).permute(1,0)\n",
        "    return mask\n",
        "  \n",
        "  def forward(self, input, input_len, target, teacher_force_ratio = 0.5):\n",
        "    '''\n",
        "    We are making the ussage of teacher-forcing technique to input the ground \n",
        "    truth 50% of times and the predicted token otherwise at each time stamp\n",
        "    input = source sentence = germany: shape = [input_len = len(germany_vocab), batch_size]\n",
        "    input_len = length of input = sequence length : shape = [batch_size]\n",
        "    target = english: shape = [target_len, batch_size]\n",
        "    '''\n",
        "    target_len = target.shape[0]\n",
        "    batch_size = input.shape[1]\n",
        "    target_voc_size = self.decoder.output_dim\n",
        "    #Create container to hold the predictions and assign it to the gpu if available\n",
        "    outputs = torch.zeros(target_len, batch_size, target_voc_size).to(self.device)\n",
        "    enc_outputs, hidden = self.encoder(input, input_len) #Run the encoder\n",
        "    #Grab the first token to enter the decoder (first input = '<sos>' token)\n",
        "    dec_input = target[0,:]\n",
        "    #get the masked input(general)\n",
        "    mask = self.mask_build(input) #shape = [batch_size, input_len]\n",
        "    #We now iterate over the target sentence (english) to do prediction one token at a time\n",
        "    for t in range(1, target_len):\n",
        "      #Here we insert in embeded input token,prev_state, enc_hidden(2),mask-->prediction, new hidden state\n",
        "      output, hidden, _ = self.decoder(dec_input, hidden, enc_outputs, mask)\n",
        "      outputs[t] = output #store the prediction at every time step\n",
        "      #We make decission if we will use teacher forcing tech or ground truth\n",
        "      teacher_force = random.random() < teacher_force_ratio\n",
        "      #Get the best guess from the output (target)\n",
        "      best_guess = output.argmax(1)\n",
        "      dec_input = target[t] if teacher_force else best_guess\n",
        "    return outputs\n"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP9iwfLkbbrd"
      },
      "source": [
        "#We can now build the training and validation loops for the above model"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hMpchUNbplT"
      },
      "source": [
        "#Hyperparameters\n",
        "input_dim = len(germany_ln.vocab)\n",
        "output_dim = len(english_ln.vocab)\n",
        "enc_embd_dim = 256\n",
        "dec_embd_dim = 256\n",
        "enc_hidden = 512\n",
        "dec_hidden = 512\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5\n",
        "learning_rate = 1e-3\n",
        "input_pad_idx = germany_ln.vocab.stoi[germany_ln.pad_token] #Get all indices for the pad-token\n",
        "attn = Attention(enc_hidden, dec_hidden)\n",
        "encoder = Encoder(input_dim, enc_embd_dim,enc_hidden,dec_hidden,enc_dropout)\n",
        "decoder = Decoder(output_dim, dec_embd_dim, enc_hidden,dec_hidden,dec_dropout,attn)\n",
        "model = AutoEncoder(encoder, decoder,input_pad_idx, device).to(device)"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG3k6OVdh_K_"
      },
      "source": [
        "#We initialize the parameters of the model(both trainable-->normal dist), and non trainable = zeros\n",
        "def wt_initializer(m):\n",
        "  for name, param in m.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      nn.init.normal_(param.data, mean = 0.0, std = 0.01)\n",
        "    else:\n",
        "      nn.init.constant_(param.data, 0)"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-7jXyKBjh2-",
        "outputId": "a490ba23-59d6-4db6-cfd5-0480054b38e4"
      },
      "source": [
        "model.apply(wt_initializer)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoEncoder(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_Lx-7B5jmgk",
        "outputId": "a2ef5df9-ea3b-424a-a32b-b66c7e822191"
      },
      "source": [
        "#We may count number of trainable parameters in the model using the following function:\n",
        "def count_params(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total number of trainable parameters in this model is: {count_params(model):,}\")"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of trainable parameters in this model is: 20,518,917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQB25VhFlEUK"
      },
      "source": [
        "#Get the optimizer and loss objects: for loss functions we are not going to encurs cost for padded indices\n",
        "pad_idx = english_ln.vocab.stoi[english_ln.pad_token] #grab those indices related to padding tokens\n",
        "loss_obj = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
        "optimizer = optim.Adam(params = model.parameters(), lr = learning_rate)"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9kjw34Ll_rM",
        "outputId": "b1d543f0-8c6d-4c68-d841-83b2d42d37d2"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AutoEncoder(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(7855, 256)\n",
            "    (rnn): GRU(256, 512, bidirectional=True)\n",
            "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (attention): Attention(\n",
            "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
            "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
            "    )\n",
            "    (embedding): Embedding(5893, 256)\n",
            "    (rnn): GRU(1280, 512)\n",
            "    (fc_out): Linear(in_features=1792, out_features=5893, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8QQY5p6xErN"
      },
      "source": [
        "#The training loop is aided by the following function:\n",
        "def train_loop(model, iterator, optimizer, loss_obj, clip):\n",
        "  model.train() #Turn on the regularization layers such as batch-norm and dropout\n",
        "  loss_per_epoch = 0\n",
        "  for i, batch in enumerate(iterator):\n",
        "    #we fetch input and target from the batch item\n",
        "    input,input_len = batch.src\n",
        "    target = batch.trg\n",
        "    optimizer.zero_grad() #initialize the grads to zero\n",
        "    output = model(input, input_len, target)\n",
        "    #target shape: [target_len, batch_size], output_dim = [target_len, batch_size, output_dim]\n",
        "    output_dim = output.shape[-1]\n",
        "    #reshape the prediction before use for cross-entropy loss: (also skip the first token (sos))\n",
        "    output = output[1:].view(-1, output_dim) #View will reshape for us = [target_len-1*batch, output_dim]\n",
        "    target = target[1:].view(-1) #shape: [target_len-1 * batch_size]\n",
        "    loss = loss_obj(output, target)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters, clip)\n",
        "    optimizer.step()\n",
        "    loss_per_epoch += loss.item()\n",
        "  return (loss_per_epoch/len(iterator))\n"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJuAWQCEDhbc"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for i, batch in enumerate(iterator):\n",
        "    src, src_len = batch.src\n",
        "    trg = batch.trg\n",
        "    optimizer.zero_grad()\n",
        "    output = model(src, src_len, trg)\n",
        "    output_dim = output.shape[-1]\n",
        "    output = output[1:].view(-1, output_dim)\n",
        "    trg = trg[1:].view(-1)\n",
        "    loss = criterion(output, trg)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLEwPv2n15dE"
      },
      "source": [
        "#Evaluation loop will be done through the following function\n",
        "def validation_loop(model, iterator, loss_obj):\n",
        "  model.eval()#turn off the regularization layers\n",
        "  loss_per_epoch = 0\n",
        "  #We do not train the model again\n",
        "  with torch.no_grad():\n",
        "    for i, batch in enumerate(iterator):\n",
        "      input, input_len = batch.src\n",
        "      target = batch.trg #shape is: [target_len, batch_size]\n",
        "      #we add 0 to turn-off teacher forcing\n",
        "      output = model(input, input_len, target, 0)# shape: [target_len, batch_size, output_dim]\n",
        "      output_dim = output.shape[-1] # grab the output dimension\n",
        "      #Reshape the output to compute cross-entropy loss: Also ignore the first token\n",
        "      output = output[1:].view(-1, output_dim) #shape = [target_len-1 * batch_size, output_dim]\n",
        "      target = target[1:].view(-1)# shape: [target_len-1* batch_size]\n",
        "      #In cross entropy loss we only need dim-0 for the predictor and actual to be the same dim\n",
        "      loss = loss_obj(output, target)\n",
        "      loss_per_epoch += loss.item()\n",
        "  return (loss_per_epoch/len(iterator))\n",
        "\n",
        "\n"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilKdZrgo8VlD",
        "outputId": "3627d346-c8d4-4dbe-81ef-1d32f1febacf"
      },
      "source": [
        "#Finaly we train our network as follows:\n",
        "num_epochs = 100\n",
        "clip = 1\n",
        "best_validation_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"\\n>>>>Training start for epoch {epoch + 1}\\n>>>>Please wait while model is training....\")\n",
        "  tic = time.time()\n",
        "  train_loss = train(model, train_iter, optimizer, loss_obj, clip)\n",
        "  validation_loss = validation_loop(model, validation_iter, loss_obj)\n",
        "  toc = time.time()\n",
        "  #saving the best weights\n",
        "  if validation_loss < best_validation_loss:\n",
        "    best_validation_loss = validation_loss\n",
        "    torch.save(model.state_dict(),'translation_with_attention.pt')\n",
        "  print(f\"\\n>>>>Epoch: {epoch + 1}: Time elapsed: {time_fmt(toc - tic)}\")\n",
        "  print(f\"\\n>>>>Training loss: {float(train_loss):.4f}\\n>>>>Validation loss: {float(validation_loss):.4f}\")\n",
        "  print(f\"\\n>>>>Training PPL: {math.exp(train_loss):7.4f}\\n>>>>Validation PPL: {math.exp(validation_loss):7.4f}\")\n",
        "\n"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>>Training start for epoch 1\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 1: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 2.6738\n",
            ">>>>Validation loss: 3.2985\n",
            "\n",
            ">>>>Training PPL: 14.4944\n",
            ">>>>Validation PPL: 27.0711\n",
            "\n",
            ">>>>Training start for epoch 2\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 2: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 2.2689\n",
            ">>>>Validation loss: 3.1946\n",
            "\n",
            ">>>>Training PPL:  9.6687\n",
            ">>>>Validation PPL: 24.3999\n",
            "\n",
            ">>>>Training start for epoch 3\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 3: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 1.9632\n",
            ">>>>Validation loss: 3.2423\n",
            "\n",
            ">>>>Training PPL:  7.1223\n",
            ">>>>Validation PPL: 25.5914\n",
            "\n",
            ">>>>Training start for epoch 4\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 4: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 1.7235\n",
            ">>>>Validation loss: 3.2237\n",
            "\n",
            ">>>>Training PPL:  5.6039\n",
            ">>>>Validation PPL: 25.1197\n",
            "\n",
            ">>>>Training start for epoch 5\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 5: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 1.5536\n",
            ">>>>Validation loss: 3.2896\n",
            "\n",
            ">>>>Training PPL:  4.7287\n",
            ">>>>Validation PPL: 26.8319\n",
            "\n",
            ">>>>Training start for epoch 6\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 6: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 1.3991\n",
            ">>>>Validation loss: 3.4178\n",
            "\n",
            ">>>>Training PPL:  4.0514\n",
            ">>>>Validation PPL: 30.5030\n",
            "\n",
            ">>>>Training start for epoch 7\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 7: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 1.2804\n",
            ">>>>Validation loss: 3.3095\n",
            "\n",
            ">>>>Training PPL:  3.5982\n",
            ">>>>Validation PPL: 27.3724\n",
            "\n",
            ">>>>Training start for epoch 8\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 8: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 1.1793\n",
            ">>>>Validation loss: 3.5413\n",
            "\n",
            ">>>>Training PPL:  3.2521\n",
            ">>>>Validation PPL: 34.5122\n",
            "\n",
            ">>>>Training start for epoch 9\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 9: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 1.0657\n",
            ">>>>Validation loss: 3.6475\n",
            "\n",
            ">>>>Training PPL:  2.9029\n",
            ">>>>Validation PPL: 38.3793\n",
            "\n",
            ">>>>Training start for epoch 10\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 10: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 1.0080\n",
            ">>>>Validation loss: 3.6637\n",
            "\n",
            ">>>>Training PPL:  2.7400\n",
            ">>>>Validation PPL: 39.0054\n",
            "\n",
            ">>>>Training start for epoch 11\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 11: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.9418\n",
            ">>>>Validation loss: 3.7184\n",
            "\n",
            ">>>>Training PPL:  2.5647\n",
            ">>>>Validation PPL: 41.2003\n",
            "\n",
            ">>>>Training start for epoch 12\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 12: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.8708\n",
            ">>>>Validation loss: 3.9036\n",
            "\n",
            ">>>>Training PPL:  2.3888\n",
            ">>>>Validation PPL: 49.5804\n",
            "\n",
            ">>>>Training start for epoch 13\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 13: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.8016\n",
            ">>>>Validation loss: 3.9973\n",
            "\n",
            ">>>>Training PPL:  2.2290\n",
            ">>>>Validation PPL: 54.4496\n",
            "\n",
            ">>>>Training start for epoch 14\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 14: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.7709\n",
            ">>>>Validation loss: 4.0044\n",
            "\n",
            ">>>>Training PPL:  2.1618\n",
            ">>>>Validation PPL: 54.8402\n",
            "\n",
            ">>>>Training start for epoch 15\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 15: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.7016\n",
            ">>>>Validation loss: 4.1666\n",
            "\n",
            ">>>>Training PPL:  2.0169\n",
            ">>>>Validation PPL: 64.4971\n",
            "\n",
            ">>>>Training start for epoch 16\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 16: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.6730\n",
            ">>>>Validation loss: 4.1303\n",
            "\n",
            ">>>>Training PPL:  1.9601\n",
            ">>>>Validation PPL: 62.1951\n",
            "\n",
            ">>>>Training start for epoch 17\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 17: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.6379\n",
            ">>>>Validation loss: 4.2133\n",
            "\n",
            ">>>>Training PPL:  1.8924\n",
            ">>>>Validation PPL: 67.5786\n",
            "\n",
            ">>>>Training start for epoch 18\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 18: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.6284\n",
            ">>>>Validation loss: 4.3149\n",
            "\n",
            ">>>>Training PPL:  1.8746\n",
            ">>>>Validation PPL: 74.8065\n",
            "\n",
            ">>>>Training start for epoch 19\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 19: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5854\n",
            ">>>>Validation loss: 4.4120\n",
            "\n",
            ">>>>Training PPL:  1.7957\n",
            ">>>>Validation PPL: 82.4381\n",
            "\n",
            ">>>>Training start for epoch 20\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 20: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5689\n",
            ">>>>Validation loss: 4.4230\n",
            "\n",
            ">>>>Training PPL:  1.7663\n",
            ">>>>Validation PPL: 83.3494\n",
            "\n",
            ">>>>Training start for epoch 21\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 21: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5644\n",
            ">>>>Validation loss: 4.4175\n",
            "\n",
            ">>>>Training PPL:  1.7584\n",
            ">>>>Validation PPL: 82.8887\n",
            "\n",
            ">>>>Training start for epoch 22\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 22: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5379\n",
            ">>>>Validation loss: 4.5359\n",
            "\n",
            ">>>>Training PPL:  1.7124\n",
            ">>>>Validation PPL: 93.3080\n",
            "\n",
            ">>>>Training start for epoch 23\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 23: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5194\n",
            ">>>>Validation loss: 4.5648\n",
            "\n",
            ">>>>Training PPL:  1.6810\n",
            ">>>>Validation PPL: 96.0467\n",
            "\n",
            ">>>>Training start for epoch 24\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 24: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5181\n",
            ">>>>Validation loss: 4.6596\n",
            "\n",
            ">>>>Training PPL:  1.6788\n",
            ">>>>Validation PPL: 105.5948\n",
            "\n",
            ">>>>Training start for epoch 25\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 25: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4940\n",
            ">>>>Validation loss: 4.7595\n",
            "\n",
            ">>>>Training PPL:  1.6389\n",
            ">>>>Validation PPL: 116.6818\n",
            "\n",
            ">>>>Training start for epoch 26\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 26: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4706\n",
            ">>>>Validation loss: 4.8024\n",
            "\n",
            ">>>>Training PPL:  1.6009\n",
            ">>>>Validation PPL: 121.8039\n",
            "\n",
            ">>>>Training start for epoch 27\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 27: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4725\n",
            ">>>>Validation loss: 4.8445\n",
            "\n",
            ">>>>Training PPL:  1.6040\n",
            ">>>>Validation PPL: 127.0342\n",
            "\n",
            ">>>>Training start for epoch 28\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 28: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4828\n",
            ">>>>Validation loss: 4.8597\n",
            "\n",
            ">>>>Training PPL:  1.6205\n",
            ">>>>Validation PPL: 128.9917\n",
            "\n",
            ">>>>Training start for epoch 29\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 29: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4608\n",
            ">>>>Validation loss: 4.8197\n",
            "\n",
            ">>>>Training PPL:  1.5854\n",
            ">>>>Validation PPL: 123.9277\n",
            "\n",
            ">>>>Training start for epoch 30\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 30: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4496\n",
            ">>>>Validation loss: 4.9468\n",
            "\n",
            ">>>>Training PPL:  1.5677\n",
            ">>>>Validation PPL: 140.7193\n",
            "\n",
            ">>>>Training start for epoch 31\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 31: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4543\n",
            ">>>>Validation loss: 5.0031\n",
            "\n",
            ">>>>Training PPL:  1.5750\n",
            ">>>>Validation PPL: 148.8727\n",
            "\n",
            ">>>>Training start for epoch 32\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 32: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4340\n",
            ">>>>Validation loss: 4.9760\n",
            "\n",
            ">>>>Training PPL:  1.5434\n",
            ">>>>Validation PPL: 144.8934\n",
            "\n",
            ">>>>Training start for epoch 33\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 33: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4345\n",
            ">>>>Validation loss: 5.0368\n",
            "\n",
            ">>>>Training PPL:  1.5442\n",
            ">>>>Validation PPL: 153.9763\n",
            "\n",
            ">>>>Training start for epoch 34\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 34: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4347\n",
            ">>>>Validation loss: 5.0309\n",
            "\n",
            ">>>>Training PPL:  1.5444\n",
            ">>>>Validation PPL: 153.0724\n",
            "\n",
            ">>>>Training start for epoch 35\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 35: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4293\n",
            ">>>>Validation loss: 5.1263\n",
            "\n",
            ">>>>Training PPL:  1.5362\n",
            ">>>>Validation PPL: 168.3891\n",
            "\n",
            ">>>>Training start for epoch 36\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 36: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4228\n",
            ">>>>Validation loss: 5.1224\n",
            "\n",
            ">>>>Training PPL:  1.5262\n",
            ">>>>Validation PPL: 167.7312\n",
            "\n",
            ">>>>Training start for epoch 37\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 37: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4153\n",
            ">>>>Validation loss: 5.1766\n",
            "\n",
            ">>>>Training PPL:  1.5148\n",
            ">>>>Validation PPL: 177.0818\n",
            "\n",
            ">>>>Training start for epoch 38\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 38: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4217\n",
            ">>>>Validation loss: 5.2175\n",
            "\n",
            ">>>>Training PPL:  1.5245\n",
            ">>>>Validation PPL: 184.4732\n",
            "\n",
            ">>>>Training start for epoch 39\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 39: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4216\n",
            ">>>>Validation loss: 5.2397\n",
            "\n",
            ">>>>Training PPL:  1.5244\n",
            ">>>>Validation PPL: 188.6101\n",
            "\n",
            ">>>>Training start for epoch 40\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 40: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4025\n",
            ">>>>Validation loss: 5.2668\n",
            "\n",
            ">>>>Training PPL:  1.4955\n",
            ">>>>Validation PPL: 193.7964\n",
            "\n",
            ">>>>Training start for epoch 41\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 41: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4186\n",
            ">>>>Validation loss: 5.3217\n",
            "\n",
            ">>>>Training PPL:  1.5198\n",
            ">>>>Validation PPL: 204.7276\n",
            "\n",
            ">>>>Training start for epoch 42\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 42: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4234\n",
            ">>>>Validation loss: 5.2174\n",
            "\n",
            ">>>>Training PPL:  1.5271\n",
            ">>>>Validation PPL: 184.4469\n",
            "\n",
            ">>>>Training start for epoch 43\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 43: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4204\n",
            ">>>>Validation loss: 5.2669\n",
            "\n",
            ">>>>Training PPL:  1.5226\n",
            ">>>>Validation PPL: 193.8060\n",
            "\n",
            ">>>>Training start for epoch 44\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 44: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4238\n",
            ">>>>Validation loss: 5.2761\n",
            "\n",
            ">>>>Training PPL:  1.5277\n",
            ">>>>Validation PPL: 195.5980\n",
            "\n",
            ">>>>Training start for epoch 45\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 45: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4178\n",
            ">>>>Validation loss: 5.3357\n",
            "\n",
            ">>>>Training PPL:  1.5187\n",
            ">>>>Validation PPL: 207.6164\n",
            "\n",
            ">>>>Training start for epoch 46\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 46: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4017\n",
            ">>>>Validation loss: 5.3194\n",
            "\n",
            ">>>>Training PPL:  1.4944\n",
            ">>>>Validation PPL: 204.2570\n",
            "\n",
            ">>>>Training start for epoch 47\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 47: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4062\n",
            ">>>>Validation loss: 5.4278\n",
            "\n",
            ">>>>Training PPL:  1.5010\n",
            ">>>>Validation PPL: 227.6438\n",
            "\n",
            ">>>>Training start for epoch 48\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 48: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.3985\n",
            ">>>>Validation loss: 5.3980\n",
            "\n",
            ">>>>Training PPL:  1.4895\n",
            ">>>>Validation PPL: 220.9631\n",
            "\n",
            ">>>>Training start for epoch 49\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 49: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4130\n",
            ">>>>Validation loss: 5.3767\n",
            "\n",
            ">>>>Training PPL:  1.5113\n",
            ">>>>Validation PPL: 216.3094\n",
            "\n",
            ">>>>Training start for epoch 50\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 50: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4217\n",
            ">>>>Validation loss: 5.4574\n",
            "\n",
            ">>>>Training PPL:  1.5246\n",
            ">>>>Validation PPL: 234.4772\n",
            "\n",
            ">>>>Training start for epoch 51\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 51: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4222\n",
            ">>>>Validation loss: 5.4148\n",
            "\n",
            ">>>>Training PPL:  1.5252\n",
            ">>>>Validation PPL: 224.7142\n",
            "\n",
            ">>>>Training start for epoch 52\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 52: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4178\n",
            ">>>>Validation loss: 5.4746\n",
            "\n",
            ">>>>Training PPL:  1.5187\n",
            ">>>>Validation PPL: 238.5652\n",
            "\n",
            ">>>>Training start for epoch 53\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 53: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4083\n",
            ">>>>Validation loss: 5.4961\n",
            "\n",
            ">>>>Training PPL:  1.5043\n",
            ">>>>Validation PPL: 243.7361\n",
            "\n",
            ">>>>Training start for epoch 54\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 54: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4114\n",
            ">>>>Validation loss: 5.4614\n",
            "\n",
            ">>>>Training PPL:  1.5090\n",
            ">>>>Validation PPL: 235.4231\n",
            "\n",
            ">>>>Training start for epoch 55\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 55: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4180\n",
            ">>>>Validation loss: 5.4927\n",
            "\n",
            ">>>>Training PPL:  1.5189\n",
            ">>>>Validation PPL: 242.9140\n",
            "\n",
            ">>>>Training start for epoch 56\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 56: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4191\n",
            ">>>>Validation loss: 5.4994\n",
            "\n",
            ">>>>Training PPL:  1.5205\n",
            ">>>>Validation PPL: 244.5537\n",
            "\n",
            ">>>>Training start for epoch 57\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 57: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4123\n",
            ">>>>Validation loss: 5.4268\n",
            "\n",
            ">>>>Training PPL:  1.5103\n",
            ">>>>Validation PPL: 227.4120\n",
            "\n",
            ">>>>Training start for epoch 58\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 58: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4166\n",
            ">>>>Validation loss: 5.4855\n",
            "\n",
            ">>>>Training PPL:  1.5167\n",
            ">>>>Validation PPL: 241.1666\n",
            "\n",
            ">>>>Training start for epoch 59\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 59: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4299\n",
            ">>>>Validation loss: 5.5854\n",
            "\n",
            ">>>>Training PPL:  1.5371\n",
            ">>>>Validation PPL: 266.5086\n",
            "\n",
            ">>>>Training start for epoch 60\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 60: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4192\n",
            ">>>>Validation loss: 5.6512\n",
            "\n",
            ">>>>Training PPL:  1.5208\n",
            ">>>>Validation PPL: 284.6379\n",
            "\n",
            ">>>>Training start for epoch 61\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 61: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4229\n",
            ">>>>Validation loss: 5.5467\n",
            "\n",
            ">>>>Training PPL:  1.5263\n",
            ">>>>Validation PPL: 256.4009\n",
            "\n",
            ">>>>Training start for epoch 62\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 62: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4235\n",
            ">>>>Validation loss: 5.5720\n",
            "\n",
            ">>>>Training PPL:  1.5273\n",
            ">>>>Validation PPL: 262.9684\n",
            "\n",
            ">>>>Training start for epoch 63\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 63: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4229\n",
            ">>>>Validation loss: 5.5688\n",
            "\n",
            ">>>>Training PPL:  1.5263\n",
            ">>>>Validation PPL: 262.1155\n",
            "\n",
            ">>>>Training start for epoch 64\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 64: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4238\n",
            ">>>>Validation loss: 5.6423\n",
            "\n",
            ">>>>Training PPL:  1.5277\n",
            ">>>>Validation PPL: 282.0981\n",
            "\n",
            ">>>>Training start for epoch 65\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 65: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4255\n",
            ">>>>Validation loss: 5.7000\n",
            "\n",
            ">>>>Training PPL:  1.5304\n",
            ">>>>Validation PPL: 298.8656\n",
            "\n",
            ">>>>Training start for epoch 66\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 66: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4333\n",
            ">>>>Validation loss: 5.6652\n",
            "\n",
            ">>>>Training PPL:  1.5424\n",
            ">>>>Validation PPL: 288.6488\n",
            "\n",
            ">>>>Training start for epoch 67\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 67: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4378\n",
            ">>>>Validation loss: 5.6266\n",
            "\n",
            ">>>>Training PPL:  1.5493\n",
            ">>>>Validation PPL: 277.7081\n",
            "\n",
            ">>>>Training start for epoch 68\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 68: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4425\n",
            ">>>>Validation loss: 5.5722\n",
            "\n",
            ">>>>Training PPL:  1.5566\n",
            ">>>>Validation PPL: 263.0062\n",
            "\n",
            ">>>>Training start for epoch 69\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 69: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4345\n",
            ">>>>Validation loss: 5.6662\n",
            "\n",
            ">>>>Training PPL:  1.5442\n",
            ">>>>Validation PPL: 288.9296\n",
            "\n",
            ">>>>Training start for epoch 70\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 70: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4367\n",
            ">>>>Validation loss: 5.6222\n",
            "\n",
            ">>>>Training PPL:  1.5476\n",
            ">>>>Validation PPL: 276.5107\n",
            "\n",
            ">>>>Training start for epoch 71\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 71: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4526\n",
            ">>>>Validation loss: 5.6596\n",
            "\n",
            ">>>>Training PPL:  1.5725\n",
            ">>>>Validation PPL: 287.0233\n",
            "\n",
            ">>>>Training start for epoch 72\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 72: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4467\n",
            ">>>>Validation loss: 5.6496\n",
            "\n",
            ">>>>Training PPL:  1.5631\n",
            ">>>>Validation PPL: 284.1782\n",
            "\n",
            ">>>>Training start for epoch 73\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 73: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4587\n",
            ">>>>Validation loss: 5.6424\n",
            "\n",
            ">>>>Training PPL:  1.5821\n",
            ">>>>Validation PPL: 282.1331\n",
            "\n",
            ">>>>Training start for epoch 74\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 74: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4395\n",
            ">>>>Validation loss: 5.6973\n",
            "\n",
            ">>>>Training PPL:  1.5520\n",
            ">>>>Validation PPL: 298.0653\n",
            "\n",
            ">>>>Training start for epoch 75\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 75: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4703\n",
            ">>>>Validation loss: 5.6318\n",
            "\n",
            ">>>>Training PPL:  1.6004\n",
            ">>>>Validation PPL: 279.1715\n",
            "\n",
            ">>>>Training start for epoch 76\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 76: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4853\n",
            ">>>>Validation loss: 5.5585\n",
            "\n",
            ">>>>Training PPL:  1.6247\n",
            ">>>>Validation PPL: 259.4369\n",
            "\n",
            ">>>>Training start for epoch 77\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 77: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4624\n",
            ">>>>Validation loss: 5.6827\n",
            "\n",
            ">>>>Training PPL:  1.5878\n",
            ">>>>Validation PPL: 293.7503\n",
            "\n",
            ">>>>Training start for epoch 78\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 78: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4624\n",
            ">>>>Validation loss: 5.6371\n",
            "\n",
            ">>>>Training PPL:  1.5878\n",
            ">>>>Validation PPL: 280.6341\n",
            "\n",
            ">>>>Training start for epoch 79\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 79: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4690\n",
            ">>>>Validation loss: 5.5993\n",
            "\n",
            ">>>>Training PPL:  1.5984\n",
            ">>>>Validation PPL: 270.2417\n",
            "\n",
            ">>>>Training start for epoch 80\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 80: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4755\n",
            ">>>>Validation loss: 5.7215\n",
            "\n",
            ">>>>Training PPL:  1.6088\n",
            ">>>>Validation PPL: 305.3488\n",
            "\n",
            ">>>>Training start for epoch 81\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 81: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4792\n",
            ">>>>Validation loss: 5.6018\n",
            "\n",
            ">>>>Training PPL:  1.6148\n",
            ">>>>Validation PPL: 270.9127\n",
            "\n",
            ">>>>Training start for epoch 82\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 82: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4855\n",
            ">>>>Validation loss: 5.7621\n",
            "\n",
            ">>>>Training PPL:  1.6250\n",
            ">>>>Validation PPL: 318.0195\n",
            "\n",
            ">>>>Training start for epoch 83\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 83: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4838\n",
            ">>>>Validation loss: 5.7185\n",
            "\n",
            ">>>>Training PPL:  1.6223\n",
            ">>>>Validation PPL: 304.4611\n",
            "\n",
            ">>>>Training start for epoch 84\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 84: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4934\n",
            ">>>>Validation loss: 5.6556\n",
            "\n",
            ">>>>Training PPL:  1.6378\n",
            ">>>>Validation PPL: 285.8924\n",
            "\n",
            ">>>>Training start for epoch 85\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 85: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4973\n",
            ">>>>Validation loss: 5.6760\n",
            "\n",
            ">>>>Training PPL:  1.6442\n",
            ">>>>Validation PPL: 291.7816\n",
            "\n",
            ">>>>Training start for epoch 86\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 86: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.4929\n",
            ">>>>Validation loss: 5.6533\n",
            "\n",
            ">>>>Training PPL:  1.6370\n",
            ">>>>Validation PPL: 285.2363\n",
            "\n",
            ">>>>Training start for epoch 87\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 87: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5159\n",
            ">>>>Validation loss: 5.6662\n",
            "\n",
            ">>>>Training PPL:  1.6751\n",
            ">>>>Validation PPL: 288.9423\n",
            "\n",
            ">>>>Training start for epoch 88\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 88: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5081\n",
            ">>>>Validation loss: 5.7580\n",
            "\n",
            ">>>>Training PPL:  1.6621\n",
            ">>>>Validation PPL: 316.7258\n",
            "\n",
            ">>>>Training start for epoch 89\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 89: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5173\n",
            ">>>>Validation loss: 5.6826\n",
            "\n",
            ">>>>Training PPL:  1.6775\n",
            ">>>>Validation PPL: 293.7201\n",
            "\n",
            ">>>>Training start for epoch 90\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 90: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5285\n",
            ">>>>Validation loss: 5.6665\n",
            "\n",
            ">>>>Training PPL:  1.6964\n",
            ">>>>Validation PPL: 289.0263\n",
            "\n",
            ">>>>Training start for epoch 91\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 91: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5068\n",
            ">>>>Validation loss: 5.7812\n",
            "\n",
            ">>>>Training PPL:  1.6599\n",
            ">>>>Validation PPL: 324.1337\n",
            "\n",
            ">>>>Training start for epoch 92\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 92: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5179\n",
            ">>>>Validation loss: 5.7218\n",
            "\n",
            ">>>>Training PPL:  1.6785\n",
            ">>>>Validation PPL: 305.4612\n",
            "\n",
            ">>>>Training start for epoch 93\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 93: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5313\n",
            ">>>>Validation loss: 5.7045\n",
            "\n",
            ">>>>Training PPL:  1.7011\n",
            ">>>>Validation PPL: 300.2018\n",
            "\n",
            ">>>>Training start for epoch 94\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 94: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5389\n",
            ">>>>Validation loss: 5.6499\n",
            "\n",
            ">>>>Training PPL:  1.7141\n",
            ">>>>Validation PPL: 284.2736\n",
            "\n",
            ">>>>Training start for epoch 95\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 95: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5389\n",
            ">>>>Validation loss: 5.6907\n",
            "\n",
            ">>>>Training PPL:  1.7141\n",
            ">>>>Validation PPL: 296.1057\n",
            "\n",
            ">>>>Training start for epoch 96\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 96: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5407\n",
            ">>>>Validation loss: 5.7648\n",
            "\n",
            ">>>>Training PPL:  1.7172\n",
            ">>>>Validation PPL: 318.8874\n",
            "\n",
            ">>>>Training start for epoch 97\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 97: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5471\n",
            ">>>>Validation loss: 5.7210\n",
            "\n",
            ">>>>Training PPL:  1.7283\n",
            ">>>>Validation PPL: 305.2066\n",
            "\n",
            ">>>>Training start for epoch 98\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 98: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5347\n",
            ">>>>Validation loss: 5.7085\n",
            "\n",
            ">>>>Training PPL:  1.7069\n",
            ">>>>Validation PPL: 301.4277\n",
            "\n",
            ">>>>Training start for epoch 99\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 99: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5521\n",
            ">>>>Validation loss: 5.7011\n",
            "\n",
            ">>>>Training PPL:  1.7368\n",
            ">>>>Validation PPL: 299.2046\n",
            "\n",
            ">>>>Training start for epoch 100\n",
            ">>>>Please wait while model is training....\n",
            "\n",
            ">>>>Epoch: 100: Time elapsed: 0: 00: 39.00\n",
            "\n",
            ">>>>Training loss: 0.5720\n",
            ">>>>Validation loss: 5.6676\n",
            "\n",
            ">>>>Training PPL:  1.7717\n",
            ">>>>Validation PPL: 289.3338\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRS1LVGWAES8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}