{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfpaZFRmt4Xgm0MsA6I7Xe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/PYTORCH/blob/main/TextLoader_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwosYwMIhduc",
        "outputId": "5f9988fa-e508-4234-8165-42ec7c54283d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            ">>>> You are on CoLaB with torch version: 2.0.0+cu118\n",
            ">>>> Available device: cpu\n",
            "/bin/bash: nvidia-smi: command not found\n",
            ">>>> Time elapsed: hrs: 0, mins: 00, secs: 06.00\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "t1 = timer()\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive/\", force_remount = True)\n",
        "  import torch\n",
        "  from torch.nn.utils.rnn import pad_sequence\n",
        "  from torch.utils.data import Dataset, DataLoader\n",
        "  import os, spacy\n",
        "  import pandas as pd\n",
        "  from torchvision import transforms\n",
        "  import matplotlib.pyplot as plt\n",
        "  from PIL import Image\n",
        "  print(f\">>>> You are on CoLaB with torch version: {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\">>>> {type(e)}: {e}\\n>>>> Please correct {type(e)} and reload the Google drive\")\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "BATCH_SIZE = 64 if device == torch.device(\"cuda\") else 32\n",
        "print(f\">>>> Available device: {device}\")\n",
        "def mytimer(t: float = timer())->float:\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"hrs: {h}, mins: {m:>02}, secs: {s:>05.2f}\"\n",
        "!nvidia-smi\n",
        "print(f\">>>> Time elapsed: {mytimer(timer() - t1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_en = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "OIDB4woVkBmd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We implement a Pytorch DataLoader for user customized dataset"
      ],
      "metadata": {
        "id": "T-sOEbJykokI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordsDictionary():\n",
        "  \"\"\"\"\"\n",
        "  This class create a word embedding conditioned\n",
        "  on minimum frequency of 5.\n",
        "  \"\"\"\"\"\n",
        "  def __init__(self,\n",
        "               min_frequency =  5):\n",
        "    self.stoi = {\"<PAD>\": 0,\n",
        "                 \"<SOS>\": 1, \n",
        "                 \"<EOS>\": 2, \n",
        "                 \"<UNK>\": 3}\n",
        "    self.itos = {0: \"<PAD>\", \n",
        "                 1: \"<SOS>\", \n",
        "                 2: \"<EOS>\", \n",
        "                 3: \"<UNK>\"}\n",
        "    self.min_frequency = min_frequency\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.stoi)\n",
        "  \n",
        "  @staticmethod\n",
        "  def mytokenizer(caption: str)->list:\n",
        "    # This method convert the text to lower case and returns the list tokens\n",
        "    return [tok.text.lower() for tok in spacy_en.tokenizer(caption)]\n",
        "  \n",
        "  def tfdif(self, caption_list):\n",
        "    \"\"\"\"\"\n",
        "    This method create a term frequency document inverse frequency dictionary\n",
        "    \"\"\"\"\"\n",
        "    terms_freq = {}\n",
        "    idx = 4 # Since we have already used 0-3 indices\n",
        "    for caption in caption_list:\n",
        "      for term in caption:\n",
        "        if term not in terms_freq:\n",
        "          terms_freq[term] = 1\n",
        "        else:\n",
        "          terms_freq[term] += 1\n",
        "        # We create the TFIDF dictionary if a term has occured at least 5 times\n",
        "        if terms_freq[term] == self.min_frequency:\n",
        "          self.stoi[term] = idx\n",
        "          self.itos[idx] = term\n",
        "\n",
        "  def word2vec(self, caption):\n",
        "    \"\"\"\"\"\n",
        "    This method convert the text into numbers\n",
        "    \"\"\"\"\"\n",
        "    tokens = self.mytokenizer(caption) # Tokenize the caption\n",
        "    \n",
        "    # We return an index of the corresponding token if min_freq == 5\n",
        "    # else we return an index of unknown word.\n",
        "    return [\n",
        "        self.stoi[tok] if tok in self.stoi else self.stoi[\"<UNK>\"]\\\n",
        "        for tok in tokens\n",
        "    ]\n",
        "  \n",
        "class PytorchTextDataReader(Dataset):\n",
        "  def __init__(self,\n",
        "               root_path: str,\n",
        "               text_path: str,\n",
        "               transform:transforms = None,\n",
        "               min_frequency: int = 5)->None:\n",
        "      \n",
        "      self.root_path = root_path\n",
        "      self.text_path = text_path\n",
        "      self.transform = transform\n",
        "      self.min_frequency = min_frequency\n",
        "      self.dfm = pd.read_csv(text_path)\n",
        "      self.images = self.dfm[\"image\"] # grab all images: pd.Series of images\n",
        "      self.captions = self.dfm[\"caption\"] # grab all captions: pd.Series of texts\n",
        "      # Instantiating the word dictionary class\n",
        "      self.mydictionary = WordsDictionary(min_frequency = self.min_frequency)\n",
        "      # Create a TFIDF instance\n",
        "      self.mydictionary.tfdif(caption_list = self.captions.tolist())\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dfm)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    img_root = self.images[index] # grab an image\n",
        "    caption = self.captions[index] # grab a caption\n",
        "    # Read an image and convert to RGB\n",
        "    img = Image.open(os.path.join(self.root_path, img_root)).convert(\"RGB\")\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    \n",
        "    # converting a caption to numeric\n",
        "    word_vec = [self.mydictionary.stoi[\"<SOS>\"]] # grab the index of a starting token in a caption\n",
        "    # adding indices for the rest of the token using word2vec method\n",
        "    word_vec += self.mydictionary.word2vec(caption = caption)\n",
        "    word_vec.append(self.mydictionary.stoi[\"<EOS>\"]) # adding an end of the text token index\n",
        "\n",
        "    word_vec = torch.tensor(word_vec) # converting into torch tensor\n",
        "\n",
        "    return img, word_vec # List of lists [[img1, caption1]....]\n",
        "  \n",
        "class MyCollate_fn():\n",
        "  \"\"\"\"\"\n",
        "  This class create a user-defined collate function\n",
        "  to padd every caption in a batch with a maximum length\n",
        "  of the caption within the respective batch. [(zero padding)]\n",
        "  \"\"\"\"\"\n",
        "  def __init__(self, pad_index):\n",
        "    self.pad_index = pad_index\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    \"\"\"\"\"\n",
        "    batch is the list of lists which contains images and embedded captions\n",
        "    [[image1, caption1], [image2, caption2],...]\n",
        "    \"\"\"\"\"\n",
        "    images = [item[0].unsqueeze(dim = 0) for item in batch] # add a batch dimension \n",
        "    images = torch.cat(images) # concatenate all images in a batch: \"must be of same shape\"\n",
        "\n",
        "    captions = [item[1] for item in batch] # grab all captions into a one list\n",
        "\n",
        "    # Padding the shorter sequences with max len of the sequence in the batch\n",
        "    padded_captions = pad_sequence(sequences = captions,\n",
        "                                  padding_value = self.pad_index,\n",
        "                                  batch_first = True)\n",
        "    \n",
        "    return images, padded_captions\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)), transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def textloader(\n",
        "    root_path: str,\n",
        "    text_path: str,\n",
        "    num_workers: int = 2,\n",
        "    pin_memmory: bool = True,\n",
        "    shuffle: bool = True,\n",
        "    batch_size: int = BATCH_SIZE):\n",
        "  \n",
        "  mytext_dataset = PytorchTextDataReader(root_path = root_path,\n",
        "                                         text_path = text_path,\n",
        "                                         transform = transform)\n",
        "  \n",
        "  pad_index = mytext_dataset.mydictionary.stoi[\"<PAD>\"] # grab the padding index\n",
        "\n",
        "  mytext_loader = DataLoader(dataset = mytext_dataset,\n",
        "                             batch_size = batch_size,\n",
        "                             shuffle = shuffle,\n",
        "                             pin_memory = pin_memmory,\n",
        "                             collate_fn = MyCollate_fn(pad_index = pad_index),\n",
        "                             num_workers = num_workers\n",
        "                             )\n",
        "  \n",
        "  return mytext_loader"
      ],
      "metadata": {
        "id": "6dToD3CQk80s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  images_path = \"/content/drive/MyDrive/flickr30k_images/flickr8k/images\"\n",
        "  texts_path = \"/content/drive/MyDrive/flickr30k_images/flickr8k/captions.txt\"\n",
        "  textloader = textloader(root_path = images_path,\n",
        "                          text_path = texts_path)\n",
        "  \n",
        "  for idx, (image, caption) in enumerate(textloader):\n",
        "    print(f\">>>> Image shape: {image.shape}\\t Caption shape: {caption.shape}\")"
      ],
      "metadata": {
        "id": "HFGNI71L2QZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1SJU-7th3xbh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}