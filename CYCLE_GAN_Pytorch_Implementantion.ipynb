{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CYCLE GAN-Pytorch Implementantion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP4JQrLOXiBw70IuyvhHNjB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/PYTORCH/blob/main/CYCLE_GAN_Pytorch_Implementantion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoTd0us-yjKD",
        "outputId": "62e474c8-7641-4fa0-a711-17307fabd462"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import torch\n",
        "  print(f\">>>> You are on CoLaB with torch version {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\">>>> {type(e)}: {e}\\n>>>> please correct {type(e)} and reload\")\n",
        "  COLAB = False\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "def time_fmt(t: float = 123.879)->float:\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"{h} hrs: {m:>02} min: {s:>05.2f} sec\"\n",
        "print(f\">>>> time formating\\tplease wait........\\n>>>> time elapsed\\t{time_fmt()}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            ">>>> You are on CoLaB with torch version 1.8.1+cu101\n",
            ">>>> time formating\tplease wait........\n",
            ">>>> time elapsed\t0 hrs: 02 min: 03.00 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfryD0H40IUb"
      },
      "source": [
        "#In this notebook we are going to implement a more advanced GAN architecture that convert one picture \n",
        "#to another and vice-versa. We bassically train 2 discriminators and 2 generators which works side by side.\n",
        "#In this case the loss function will be splitted into 6 quantities related to generators, discriminators, identity and cycle.\n",
        " "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD8o_Z-_1R6e"
      },
      "source": [
        "#Importing modules and setup the seed-value and the device to deterministic:\n",
        "import torch, os, copy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import PIL\n",
        "import math, random, sys, time\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensor\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbsMGJ8o2Z1k"
      },
      "source": [
        "#Seting the seed-values for reporoducability and the device to avoid errors during training \n",
        "seed = 1234\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRPFKi212vIi"
      },
      "source": [
        "#Model building involving the ussual structure of discriminator and generator classes:\n",
        "#In CYCLE-GAN-the discriminator has a CNN with a scalar outputs (to discriminate fake-real)\n",
        "#The generator will have an architecture similar to that of a U-NET (i.e; both down-sampling and upsampling)\n",
        "#conv-layers to construct the images:"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rev8e1xG3w_-"
      },
      "source": [
        "###########################Generator class ##############################Generator class ################################"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqQP_7m5C_7H"
      },
      "source": [
        "#We first prepare the convolution block to re-use it later when defining the skip-block\n",
        "class ConBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels,down = True, act_use = True, **kwargs):\n",
        "    super(ConBlock, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, padding_mode = 'reflect', **kwargs)\n",
        "        if down\n",
        "        else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
        "        nn.InstanceNorm2d(out_channels),\n",
        "        nn.ReLU(inplace = True) if act_use else nn.Identity()\n",
        "    )\n",
        "  \n",
        "  def forward(self, input_tensor):\n",
        "    return self.conv(input_tensor)\n",
        "\n",
        "#We create the residual block with this class.\n",
        "class SkipBlock(nn.Module):\n",
        "  def __init__(self, channels):\n",
        "    super(SkipBlock, self).__init__()\n",
        "    self.block = nn.Sequential(\n",
        "        ConBlock(channels, channels, kernel_size = 3, padding = 1),\n",
        "        ConBlock(channels, channels, act_use = False, kernel_size = 3, padding = 1))\n",
        "  \n",
        "  def forward(self, input_tensor):\n",
        "    return input_tensor + self.block(input_tensor)\n",
        "\n",
        "#The actual generator class which reuse the above convblock and resblock classes:\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, img_channels, num_features = 64, num_skip = 9):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    self.initial_block = nn.Sequential(\n",
        "        nn.Conv2d(img_channels, num_features, kernel_size = 7, stride = 1, padding_mode = 'reflect', padding = 3 ),\n",
        "        nn.InstanceNorm2d(num_features),\n",
        "        nn.ReLU(inplace = True))\n",
        "    \n",
        "    self.down_blocks = nn.ModuleList(\n",
        "      [\n",
        "       ConBlock(num_features, 2*num_features, kernel_size = 3, stride = 2, padding = 1),\n",
        "       ConBlock(2*num_features, 4*num_features, kernel_size = 3, stride = 2, padding = 1)\n",
        "      ])\n",
        "    self.resblocks = nn.Sequential(\n",
        "      *[SkipBlock(4*num_features) for _ in range(num_skip)])\n",
        "    \n",
        "    self.up_blocks = nn.ModuleList(\n",
        "      [ConBlock(4*num_features, 2*num_features,down = False, kernel_size = 3, stride = 2, padding = 1, output_padding = 1),\n",
        "      ConBlock(2*num_features, 1*num_features, down = False, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)])\n",
        "    \n",
        "    self.final = nn.Conv2d(num_features, img_channels, kernel_size = 7,stride = 1, padding = 3, padding_mode = 'reflect')\n",
        "  \n",
        "  def forward(self, input_tensor):\n",
        "    x = self.initial_block(input_tensor)\n",
        "    for layer in self.down_blocks:\n",
        "      x = layer(x)\n",
        "    x = self.resblocks(x)\n",
        "    for layer in self.up_blocks:\n",
        "      x = layer(x)\n",
        "    return torch.tanh(self.final(x))\n",
        "\n",
        "################End of Generator class ########### End of Generator class############ End of Generator Class###############################"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c6a0SiQQg9H"
      },
      "source": [
        "#Testing the generator if its generate the intended image:\n",
        "def __test__():\n",
        "  img_channels = 3\n",
        "  H = 256\n",
        "  W = 256\n",
        "  batch_size = 32\n",
        "  rand_img = torch.randn(batch_size, img_channels, W, H).to(device = device)\n",
        "  generator = Generator(img_channels,9).to(device = device)\n",
        "  gen_output = generator(rand_img)\n",
        "  return f\"gen_output_shape: {gen_output.shape}\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ArhjaapbRm2A",
        "outputId": "ab241e50-7fd0-42e7-9aff-84ffd426c9e7"
      },
      "source": [
        "__test__()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gen_output_shape: torch.Size([32, 3, 256, 256])'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Cgd0rYRpzQ"
      },
      "source": [
        "\n",
        "#########Discriminator class ########Discriminator class #############Discriminator class########################## DISCRIMINATOR#############\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edQtY21Nl0mz"
      },
      "source": [
        "#We first create a convolution block to re-use later when defining the discriminatoe\n",
        "class DConBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride):\n",
        "    super(DConBlock, self).__init__()\n",
        "    self.convd = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 4, 1, bias = True, padding_mode = 'reflect'),\n",
        "        nn.InstanceNorm2d(out_channels),\n",
        "        nn.LeakyReLU(0.2, inplace = True))\n",
        "    \n",
        "  def forward(self, input_tensor):\n",
        "    return self.convd(input_tensor)\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlJzjBJEn9QQ"
      },
      "source": [
        "#We then use the above conv-block to create Discriminator network:\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channels = 3, num_features = [64, 128, 256, 512]):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.first_block = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, num_features[0], kernel_size = 4, stride = 2, padding = 1, padding_mode = 'reflect'),\n",
        "        nn.LeakyReLU(0.2, inplace = True))\n",
        "    layers = []\n",
        "    in_channels = num_features[0] #updating for the next conv-layer\n",
        "    #We start iteration from the 2nd feature\n",
        "    for feature in num_features[1:]:\n",
        "      layers.append(DConBlock(in_channels, feature, stride = 1 if feature == num_features[-1] else 2))\n",
        "      in_channels = feature #update again for the secong layer\n",
        "    layers.append(nn.Conv2d(in_channels, 1, kernel_size = 4, stride = 1, padding = 1, padding_mode = 'reflect'))\n",
        "      #Unpacking the layers and create a model object\n",
        "    self.model = nn.Sequential(*layers)\n",
        "  \n",
        "  def forward(self, input_tensor):\n",
        "    x = self.first_block(input_tensor)\n",
        "    return torch.sigmoid(self.model(x)) #make sure the output is a binary\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWMwEmyeueWx"
      },
      "source": [
        "#Testing the Discriminator with the random generated noise\n",
        "def __dtest__():\n",
        "  H,W = 256, 256\n",
        "  img_channels = 3\n",
        "  batch_size = 64\n",
        "  noise = torch.randn(batch_size, img_channels, W,H).to(device = device)\n",
        "  discriminator = Discriminator().to(device = device)\n",
        "  dis_out = discriminator(noise)\n",
        "  return f\"discriminator-output_shape: {dis_out.shape}\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wTDu5X0nv6xN",
        "outputId": "d79d66a1-c201-4b4c-e600-6345552b7ccd"
      },
      "source": [
        "__dtest__()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'discriminator-output_shape: torch.Size([64, 1, 118, 118])'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXsZaCqYqCtA"
      },
      "source": [
        "###################### End Discriminator ################# End Discriminator ########################## End Discriminator ####################"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93RVlP1syHr8"
      },
      "source": [
        "###Defining the checkpoint to save and load the weights ######\n",
        "def save_chkpt(model, optimizer, file_name = \"cycle_gan.path.tar\"):\n",
        "  print(f\">>>> saving the checkpoint.........\")\n",
        "  checkpoint =  {'state_dict': model.state_dict(),\n",
        "                 'optimizers': optimizer.state_dict()}\n",
        "  torch.save(checkpoint, file_name)\n",
        "\n",
        "def load_checkpoint(model, optimizer, lr, check_point_file):\n",
        "  print(f\">>>> loading checkpoint..........\")\n",
        "  checkpoint = torch.load(check_point_file, map_location = device)\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  for par_group in optimizer.param_groups:\n",
        "    par_group['lr'] = lr"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcwCS1CdPuEP"
      },
      "source": [
        "############ Loading and preprocess Data ############### Loading and preprocess Data ################# Loading Preprocess #######################\n",
        "##******************We are going to use the dataset of horses and zebras from Kaggle*************"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPc-6KTgc0ag"
      },
      "source": [
        "#HYPER PARAMETERS\n",
        "EPOCHS = 5\n",
        "batch_size = 1\n",
        "learning_rate = 1e-5\n",
        "lambda_ID = 0.0 # for identity loss component\n",
        "lambda_cycle = 10 # for cycle loss component\n",
        "num_workers = 4 # for the effectiveness of loader\n",
        "train_dir = \"/content/drive/MyDrive/GANS/archive (4)\"\n",
        "val_dir = \"/content/drive/MyDrive/GANS/archive (4)\"\n",
        "gen_h_ckpt = \"genh.path.tar\" #checkpoint for generator(horse-training)\n",
        "gen_z_ckpt = \"genz.path.tar\" #checkpoint for generator(zebra-training)\n",
        "dis_h_ckpt = \"dish.path.tar\" #checkpoint for discriminator(horse-training)\n",
        "dis_z_ckpt = \"disz.path.tar\" #checkpoint for discriminator(zebra-training)\n",
        "load_model = True\n",
        "save_model = True\n",
        "\n",
        "transforms = A.Compose(\n",
        "    [\n",
        "     A.Resize(height = 256, width = 256), A.VerticalFlip(), \n",
        "     A.Normalize(mean = [0.5,0.5,0.5], \n",
        "     std = [0.5,0.5,0.5], max_pixel_value = 255), ToTensor()\n",
        "    ],\n",
        "    additional_targets = {'image0':'image'}\n",
        ")\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMKBIR7KipbN"
      },
      "source": [
        "###### Loading and preprocess the dataset from the directory################ Loading and Preprocess the dataset ###########################\n",
        "class HZDataset(Dataset):\n",
        "  def __init__(self, root_zebra, root_horse, transform = None):\n",
        "    self.root_zebra = root_zebra\n",
        "    self.root_horse = root_horse\n",
        "    self.transform = transform\n",
        "    self.zebra_imgs = os.listdir(root_zebra)\n",
        "    self.horse_imgs = os.listdir(root_horse)\n",
        "    self.dfm_len = max(len(self.zebra_imgs), len(self.horse_imgs)) #since the two sets are not of equal length\n",
        "    self.zebra_len = len(self.zebra_imgs) #grab number of rows for the zebra dataset\n",
        "    self.horse_len = len(self.horse_imgs) #grab number of rows for the horse dataset\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.dfm_len\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    '''make sure we do not go out of range'''\n",
        "    zebra_imgs = self.zebra_imgs[index % self.zebra_len]\n",
        "    horse_imgs = self.horse_imgs[index % self.horse_len]\n",
        "    zebra_path = os.path.join(self.root_zebra, zebra_imgs)\n",
        "    horse_path = os.path.join(self.root_horse, horse_imgs)\n",
        "    zebra_image = np.array(PIL.Image.open(zebra_path).convert('RGB'))\n",
        "    horse_image = np.array(PIL.Image.open(horse_path).convert('RGB'))\n",
        "    if self.transform:\n",
        "      dfm_aug = self.transform(image = zebra_image, image0 = horse_image)\n",
        "      zebra_image = dfm_aug['image']\n",
        "      horse_image = dfm_aug['image0']\n",
        "    return zebra_image, horse_image\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLCjsNQBomxI"
      },
      "source": [
        "#The training Loop: This is an important and longest function of cycle GAN!!!!!\n",
        "#Lets get started#####"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzS9U90AyVNb"
      },
      "source": [
        "def train_loop(disc_h, disc_z, gen_h, gen_z, loader, opt_gen, opt_disc,l1, mse, d_scaler, g_scaler):\n",
        "  h_fakes = 0\n",
        "  h_reals = 0\n",
        "  mytqdm = tqdm(loader, leave = True)\n",
        "  for idx, (zebra, horse) in enumerate(mytqdm):\n",
        "    zebra = zebra.to(device = device)\n",
        "    horse = horse.to(device = device)\n",
        "\n",
        "    #Traning the discriminators (both for horse & zebra)\n",
        "    with torch.cuda.amp.autocast():\n",
        "      fake_horse = gen_h(zebra)\n",
        "      d_h_real = disc_h(horse)\n",
        "      d_h_fake = disc_h(fake_horse.detach()) #detach to allow this component to be free after backward pass for later use\n",
        "      h_reals+=d_h_real.mean().item() #\n",
        "      h_fakes+=d_h_fake.mean().item()\n",
        "      d_h_real_loss = mse(d_h_real, torch.ones_like(d_h_real)) #discriminator loss due to real horse\n",
        "      d_h_fake_loss = mse(d_h_fake, torch.zeros_like(d_h_fake))#discriminator loss due to fake horse\n",
        "      d_h_loss = (d_h_real_loss + d_h_fake_loss)/2\n",
        "      #discriminator for zebra\n",
        "      fake_zebra = gen_z(horse)\n",
        "      d_z_real = disc_z(zebra)\n",
        "      d_z_fake = disc_z(fake_zebra.detach())\n",
        "      d_z_real_loss = mse(d_z_real, torch.ones_like(d_z_real))\n",
        "      d_z_fake_loss = mse(d_z_fake, torch.zeros_like(d_z_fake))\n",
        "      d_z_loss = (d_z_real_loss + d_z_fake_loss)/2\n",
        "      #total discriminator loss\n",
        "      d_loss = (d_h_loss + d_z_loss) / 2\n",
        "    \n",
        "    opt_disc.zero_grad()#initialize the discriminator slope parameters to zeros\n",
        "    d_scaler.scale(d_loss).bakward() # backward pass for the discriminator\n",
        "    d_scaler.step(opt_disc) #gradient descent \n",
        "    d_scaler.update()\n",
        "\n",
        "    #We now train the generators (both for horse and zebras):\n",
        "    with torch.cuda.amp.autocast():\n",
        "      d_h_fake = disc_h(fake_horse)\n",
        "      d_z_fake = disc_z(fake_zebra)\n",
        "      #We fool both discriminators for horse and zebra\n",
        "      loss_gen_horse = mse(d_h_fake, torch.ones(d_h_fake))\n",
        "      loss_gen_zebra = mse(d_z_fake, torch.ones(d_z_fake))\n",
        "\n",
        "      #cycle loss: penalty between a real horse and a fake horse (l1-norm)\n",
        "      cycle_zebra = gen_z(fake_horse)\n",
        "      cycle_horse = gen_h(fake_zebra)\n",
        "      cycle_z_loss = l1(zebra, cycle_zebra) # distance btn a fake horse and a real horse\n",
        "      cycle_h_loss = l1(horse, cycle_horse)\n",
        "\n",
        "      #Identity loss: penalty between a real horse/zebra and a generated one (l1-norm)\n",
        "      id_zebra = gen_z(zebra)\n",
        "      id_horse = gen_h(horse)\n",
        "      id_zebra_loss = l1(zebra, id_zebra)\n",
        "      id_horse_loss = l1(horse, id_zebra)\n",
        "\n",
        "      #total generator loss (adding all of them)\n",
        "      gen_loss = (loss_gen_horse + \n",
        "                  loss_gen_zebra + \n",
        "                  cycle_z_loss * lambda_cycle + \n",
        "                  cycle_h_loss * lambda_cycle + \n",
        "                  id_zebra_loss * lambda_ID + \n",
        "                  id_horse_loss * lambda_ID)\n",
        "      \n",
        "      opt_gen.zero_grad() #initialize the grad parameters to zero\n",
        "      g_scaler.scale(gen_loss).backward() #bakward pass for the generator\n",
        "      g_scaler.step(opt_gen) # gradient descent for the generator\n",
        "      g_scaler.update()\n",
        "      if idx % 100 == 0:\n",
        "        save_image(fake_horse * 0.5 + 0.5, f\"saved_images/horse_{idx}.png\")\n",
        "        save_image(fake_zebra * 0.5 + 0.5, f\"saved_images/zebra_{idx}.png\")\n",
        "        \n",
        "      mytqdm.set_postfix(h_real = h_real/(idx + 1), h_fake = h_fake/(idx + 1))\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV1MaCesZFqS"
      },
      "source": [
        "##########Finally we train our Model using the following function####################"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onTPjI9FZepN"
      },
      "source": [
        "def __train__():\n",
        "  global_tic = time.time()\n",
        "  #instantiate the model classes\n",
        "  disc_h = Discriminator(in_channels = 3).to(device = device)\n",
        "  disc_z = Discriminator(in_channels = 3).to(device = device)\n",
        "  gen_h = Generator(img_channels = 3, num_skip = 9).to(device = device)\n",
        "  gen_z = Generator(img_channels = 3, num_skip = 9).to(device = device)\n",
        "\n",
        "  #get the optimizers objects\n",
        "  opt_gen = optim.Adam(params = list(gen_h.parameters()) + list(gen_z.parameters()),\n",
        "                       lr = learning_rate, betas = (0.5, 0.999))\n",
        "  opt_disc = optim.Adam(params = list(disc_h.parameters()) + list(disc_z.parameters()),\n",
        "                        lr = learning_rate, betas = (0.5, 0.999))\n",
        "  \n",
        "  #Get the loss object (norm1 and mse)\n",
        "  l1 = nn.L1Loss()\n",
        "  mse = nn.MSELoss()\n",
        "\n",
        "  #loading the check points if any.\n",
        "  if not load_model:\n",
        "    load_checkpoint(gen_h_ckpt, gen_h, opt_gen, learning_rate)\n",
        "    load_checkpoint(gen_z_ckpt, gen_z, opt_gen, learning_rate)\n",
        "    load_checkpoint(dis_h_ckpt, disc_h, opt_disc, learning_rate)\n",
        "    load_checkpoint(dis_z_ckpt, disc_z, opt_disc, learning_rate)\n",
        "  \n",
        "  #importing and processing the dataset (using the above defined function)\n",
        "  train_data = HZDataset(root_zebra = train_dir+\"/trainB\",root_horse = train_dir+\"/trainA\")\n",
        "  val_data = HZDataset(root_zebra = val_dir+\"/testB\", root_horse = val_dir+\"/testA\")\n",
        "\n",
        "  #defining the loaders\n",
        "  loader = DataLoader(\n",
        "      dataset = train_data, \n",
        "      shuffle = True,\n",
        "       batch_size = batch_size, \n",
        "       num_workers = num_workers, \n",
        "       pin_memory = True)\n",
        "  \n",
        "  val_loader = DataLoader(\n",
        "      dataset = val_data,\n",
        "      shuffle = False,\n",
        "      batch_size = batch_size,\n",
        "      pin_memory = True)\n",
        "  \n",
        "  #get the gradient scaler object\n",
        "  g_scaler = torch.cuda.amp.GradScaler() # for generator\n",
        "  d_scaler = torch.cuda.amp.GradScaler() # for discriminator\n",
        "\n",
        "  #Actual Training is going down here!!!!!!!!!!!\n",
        "  for epoch in range(EPOCHS):\n",
        "    tic = time.time()\n",
        "    print(f\"\\n>>>> training starts for the epoch {epoch + 1}\\tplease wait..........\")\n",
        "    train_loop(disc_h, disc_z, gen_h, gen_z, loader, opt_gen, opt_disc, l1,mse,d_scaler, g_scaler)\n",
        "    toc = time.time()\n",
        "    #saving check-points\n",
        "    if save_model:\n",
        "      save_chkpt(gen_h, opt_gen, file_name = gen_h_ckpt)\n",
        "      save_chkpt(gen_z, opt_gen, file_name = gen_z_ckpt)\n",
        "      save_chkpt(disc_h, opt_disc, file_name = dis_h_ckpt)\n",
        "      save_chkpt(disc_z, opt_disc, file_name = dis_z_ckpt)\n",
        "    print(f\"\\n>>>> time elapsed at the end of epoch {epoch + 1}:{time_fmt(toc - tic)}\")\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVPjHVDroBaY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}